"""
–û—Å–Ω–æ–≤–Ω–æ–π –º–æ–¥—É–ª—å —Å–∫–∞–Ω–µ—Ä–∞ —É—è–∑–≤–∏–º–æ—Å—Ç–µ–π.
–≠—Ç–æ—Ç —Ñ–∞–π–ª —Å–ª—É–∂–∏—Ç —Ç–æ—á–∫–æ–π –≤—Ö–æ–¥–∞ –∏ –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –≤—Å–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã —Å–∏—Å—Ç–µ–º—ã.
"""

# –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏
import gc
import re
import os
import time
import asyncio
import hashlib
from typing import Dict, List, Optional, Any, Set, Tuple, TypedDict
from datetime import datetime
from functools import lru_cache
from urllib.parse import urlparse, urljoin, parse_qs, urlencode

# –°—Ç–æ—Ä–æ–Ω–Ω–∏–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏
import aiohttp
from bs4 import BeautifulSoup
from bs4.element import Tag
from PyQt5.QtCore import pyqtSignal, QObject

# –í–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ –º–æ–¥—É–ª–∏
from .cache_manager import TTLCache, cache_manager
from utils.logger import logger
from utils.database import db
from utils.unified_error_handler import log_and_notify
from utils.performance import get_local_timestamp
from utils.security import is_safe_url, validate_input_length

__all__ = ['cache_manager', 'TTLCache', 'Scanner', 'ScanWorker', 'SQL_ERROR_PATTERNS', 'XSS_REFLECTED_PATTERNS', 'SAFE_SQL_PAYLOADS', 'SAFE_XSS_PAYLOADS']

# –û—á–∏—Å—Ç–∫–∞ –∫—ç—à–∞ –ø—Ä–∏ –∏–º–ø–æ—Ä—Ç–µ –º–æ–¥—É–ª—è
cache_manager.cleanup_all()

# –ì–ª–æ–±–∞–ª—å–Ω—ã–µ –∫—ç—à–∏
HTML_CACHE = TTLCache(maxsize=1000, ttl=3600)
DNS_CACHE = TTLCache(maxsize=500, ttl=1800)
FORM_HASH_CACHE = TTLCache(maxsize=2000, ttl=7200)
URL_PROCESSING_CACHE = TTLCache(maxsize=5000, ttl=3600)

# –ö–æ–Ω—Å—Ç–∞–Ω—Ç—ã
DEFAULT_HTML_PARSER = 'html.parser'
MAX_RETRIES = 3
REQUEST_TIMEOUT = 30
MAX_CONCURRENT_REQUESTS = 5
MAX_PAYLOADS_PER_URL = 40
MAX_DEPTH = 3

# –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ HTTP
HTTP_OPTIMIZATIONS: Dict[str, Any] = {
    'timeout': {
        'total': 30,
        'connect': 10,
        'sock_read': 30,
        'sock_connect': 10
    },
    'headers': {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
        'Accept-Language': 'en-US,en;q=0.5',
        'Accept-Encoding': 'gzip, deflate',
        'Connection': 'keep-alive',
        'Upgrade-Insecure-Requests': '1'
    }
}

# –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è —É—è–∑–≤–∏–º–æ—Å—Ç–µ–π
SQL_ERROR_PATTERNS = [
    re.compile(r"sql", re.IGNORECASE),
    re.compile(r"mysql", re.IGNORECASE),
    re.compile(r"unclosed quotation mark", re.IGNORECASE),
    re.compile(r"syntax error", re.IGNORECASE),
    re.compile(r"database error", re.IGNORECASE),
    re.compile(r"invalid query", re.IGNORECASE)
]

XSS_REFLECTED_PATTERNS = [
    re.compile(r"<script>alert\('XSS'\)</script>", re.IGNORECASE),
    re.compile(r"<svg/onload=alert\('XSS'\)>", re.IGNORECASE)
]

# –ü—ç–π–ª–æ–∞–¥—ã –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
SAFE_XSS_PAYLOADS = [
    "<script>alert('XSS')</script>",
    "<img src=x onerror=alert(1)>",
    "<svg onload=alert(1)>",
    '<img src=x onerror=alert(document.domain)>',
    '<img src=x onerror=alert(document.cookie)>',
    '<body onload=alert(1)>',
    '<input onfocus=alert(1) autofocus>',
    '<iframe src=javascript:alert(1)>',
    '<a href=javascript:alert(1)>Click</a>',
    '<form><button formaction="javascript:alert(1)">X</button></form>'
]

SAFE_SQL_PAYLOADS = [
    "'", '"', "`",
    "' OR '1'='1 -- ",
    '" OR "1"="1" -- ',
    "1' OR 1=1--",
    "1' OR 'a'='a' -- ",
    "admin' -- ",
    "' OR SLEEP(5)--",
    "' UNION SELECT NULL,NULL--",
    "' AND 1=(SELECT COUNT(*) FROM tabname);-- ",
    "' OR TRUE-- ",
    "'/**/OR/**/1=1-- ",
    "' OR 'a'='a'-- "
]

SAFE_CSRF_PAYLOADS = [
    '<form action="/target" method="POST"><input type="hidden" name="amount" value="1000"></form>',
    '<img src="http://target.site/transfer?amount=1000&to=attacker">',
    '<script>fetch("/target",{method:"POST",body:"amount=1000"})</script>',
    '<iframe src="http://target.site/transfer?amount=1000&to=attacker"></iframe>'
]

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è
FORM_INPUT_TYPES = {'text', 'textarea', 'password', 'email', 'search', 'url', 'tel', 'number'}
EXCLUDED_EXTENSIONS = {'.css', '.js', '.png', '.jpg', '.jpeg', '.gif', '.svg', '.ico', '.woff', '.woff2'}
SKIP_EXTENSIONS = {
    '.jpg', '.jpeg', '.png', '.gif', '.bmp', '.svg', '.ico',
    '.pdf', '.doc', '.docx', '.xls', '.xlsx', '.ppt', '.pptx',
    '.zip', '.rar', '.7z', '.tar', '.gz',
    '.mp3', '.wav', '.mp4', '.avi', '.mov', '.mkv',
    '.exe', '.dll', '.bin', '.iso'
}

@lru_cache(maxsize=100)
def parse_html_cached(html: str, parser: str = DEFAULT_HTML_PARSER):
    """–ö—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ HTML."""
    return BeautifulSoup(html, parser)

def is_file_url(url: str) -> bool:
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ URL —Ñ–∞–π–ª–æ–º –ø–æ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—é."""
    path = urlparse(url).path
    _, ext = os.path.splitext(path)
    return ext.lower() in SKIP_EXTENSIONS

class VulnerabilityResult(TypedDict, total=False):
    type: str
    url: str
    payload: str
    vulnerability_type: str
    description: str
    severity: str
    details: str
    timestamp: str

class ScanCompletionMetrics(TypedDict):
    errors_encountered: int
    urls_scanned: int
    vulnerabilities_found: int
    status: str

ScanResults = List[VulnerabilityResult]

class Scanner(QObject):
    """–û—Å–Ω–æ–≤–Ω–æ–π –∫–ª–∞—Å—Å —Å–∫–∞–Ω–µ—Ä–∞ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏."""
    
    # –°–∏–≥–Ω–∞–ª—ã
    scan_started = pyqtSignal(str)
    scan_finished = pyqtSignal(str)
    error_occurred = pyqtSignal(str)
    vulnerability_found = pyqtSignal(str, str, str)

    def __init__(self) -> None:
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–∫–∞–Ω–µ—Ä–∞."""
        super().__init__()
        self._initialize_state()
    
    def _initialize_state(self) -> None:
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–æ—Å—Ç–æ—è–Ω–∏—è —Å–∫–∞–Ω–µ—Ä–∞."""
        self._scan_in_progress = False
        self._scan_results: ScanResults = []
        self._current_url = ""
        self._scan_id = hashlib.md5(str(time.time()).encode()).hexdigest()
        self._scan_options = {
            'max_depth': MAX_DEPTH,
            'timeout': REQUEST_TIMEOUT,
            'max_retries': MAX_RETRIES,
            'concurrent_requests': MAX_CONCURRENT_REQUESTS
        }
        self._scan_start_time = None
        self._scan_end_time = None
        self.should_stop = False
        self._is_paused = False

    @property
    def scan_in_progress(self) -> bool:
        return self._scan_in_progress

    @scan_in_progress.setter
    def scan_in_progress(self, value: bool) -> None:
        self._scan_in_progress = value

    def stop(self) -> None:
        """–û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ."""
        self.should_stop = True

    def pause(self) -> None:
        """–ü—Ä–∏–æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ."""
        self._is_paused = True

    def resume(self) -> None:
        """–í–æ–∑–æ–±–Ω–æ–≤–ª—è–µ—Ç —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ."""
        self._is_paused = False

    def is_paused(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –ª–∏ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ –ø–∞—É–∑–µ."""
        return self._is_paused

    async def _perform_scan(self) -> None:
        """–í—ã–ø–æ–ª–Ω—è–µ—Ç –æ—Å–Ω–æ–≤–Ω–æ–µ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ."""
        if not db.is_valid_url(self._current_url):
            raise ValueError("Invalid URL")

        # –û—Å–Ω–æ–≤–Ω—ã–µ –ø—Ä–æ–≤–µ—Ä–∫–∏ –Ω–∞ —É—è–∑–≤–∏–º–æ—Å—Ç–∏
        await self._check_sql_injections()
        await self._check_xss_reflected()
        await self._check_csrf_vulnerabilities()

    async def _check_sql_injections(self) -> None:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ SQL –∏–Ω—ä–µ–∫—Ü–∏–∏."""
        for payload in SAFE_SQL_PAYLOADS[:MAX_PAYLOADS_PER_URL]:
            if self.should_stop or self._is_paused:
                return
            await self._test_payload(payload, "SQL Injection")

    async def _check_xss_reflected(self) -> None:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –æ—Ç—Ä–∞–∂–µ–Ω–Ω—ã–π XSS."""
        for payload in SAFE_XSS_PAYLOADS[:MAX_PAYLOADS_PER_URL]:
            if self.should_stop or self._is_paused:
                return
            await self._test_payload(payload, "Reflected XSS")

    async def _check_csrf_vulnerabilities(self) -> None:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ CSRF —É—è–∑–≤–∏–º–æ—Å—Ç–∏."""
        for payload in SAFE_CSRF_PAYLOADS[:MAX_PAYLOADS_PER_URL]:
            if self.should_stop or self._is_paused:
                return
            await self._test_payload(payload, "CSRF")

    async def _test_payload(self, payload: str, vulnerability_type: str) -> None:
        """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –ø—ç–π–ª–æ–∞–¥–∞."""
        try:
            response = await self._send_request_with_payload(payload)
            if response and await self._is_vulnerable(response, payload, vulnerability_type):
                self.vulnerability_found.emit(self._current_url, payload, vulnerability_type)
                
        except Exception as e:
            logger.error(f"Error testing payload {payload}: {str(e)}")

    async def _send_request_with_payload(self, payload: str) -> Optional[aiohttp.ClientResponse]:
        """–û—Ç–ø—Ä–∞–≤–∫–∞ HTTP –∑–∞–ø—Ä–æ—Å–∞ —Å –ø—ç–π–ª–æ–∞–¥–æ–º."""
        if self.should_stop or self._is_paused or not self._current_url:
            return None

        timeout = aiohttp.ClientTimeout(total=self._scan_options['timeout'])
        
        try:
            async with aiohttp.ClientSession(timeout=timeout) as session:
                if '?' in self._current_url:
                    url = f"{self._current_url}&payload={payload}"
                else:
                    url = f"{self._current_url}?payload={payload}"
                
                async with session.get(url) as response:
                    return response
        except Exception as e:
            logger.debug(f"Request failed for {payload}: {e}")
            return None


    @staticmethod
    async def _is_vulnerable(response: aiohttp.ClientResponse, payload: str, vulnerability_type: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –æ—Ç–≤–µ—Ç —É—è–∑–≤–∏–º—ã–º."""
        content = await response.text()
        
        if vulnerability_type == "SQL Injection":
            return any(pattern.search(content) for pattern in SQL_ERROR_PATTERNS)
        elif vulnerability_type == "Reflected XSS":
            return payload in content
        elif vulnerability_type == "CSRF":
            return "csrf" not in content.lower()
            
        return False

    @staticmethod
    def _generate_scan_id() -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —É–Ω–∏–∫–∞–ª—å–Ω–æ–≥–æ ID —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è."""
        return hashlib.sha256(str(time.time()).encode()).hexdigest()

    async def save_scan_results(self) -> None:
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö."""
        try:
            duration = (self._scan_end_time - self._scan_start_time).total_seconds() if self._scan_end_time and self._scan_start_time else 0.0
            
            db_results: List[Dict[str, str]] = []
            
            # –ï—Å–ª–∏ —É—è–∑–≤–∏–º–æ—Å—Ç–µ–π –Ω–µ –Ω–∞–π–¥–µ–Ω–æ, –¥–æ–±–∞–≤–ª—è–µ–º –∑–∞–ø–∏—Å—å –æ–± —ç—Ç–æ–º
            if not self._scan_results:
                db_results.append({
                    'type': 'info',
                    'url': self._current_url,
                    'details': '–°–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ. –£—è–∑–≤–∏–º–æ—Å—Ç–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã.',
                    'severity': 'info'
                })
            else:
                for result in self._scan_results:
                    db_results.append({
                        'type': result.get('vulnerability_type', 'unknown'),
                        'url': result.get('url', self._current_url),
                        'details': result.get('description', ''),
                        'severity': result.get('severity', 'medium')
                    })
                
            scan_type = self._scan_options.get('type', 'general')
            if not isinstance(scan_type, str):
                scan_type = str(scan_type)
            
            db.save_scan_async(
                user_id=int(self._scan_id, 16),
                url=self._current_url,
                results=db_results,
                scan_type=scan_type,
                scan_duration=duration
            )
        except Exception as e:
            logger.error(f"Error saving scan results: {str(e)}")

class ScanWorkerSignals(QObject):
    """–°–∏–≥–Ω–∞–ª—ã –¥–ª—è –≤–æ—Ä–∫–µ—Ä–∞ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è."""
    result = pyqtSignal(dict)
    progress = pyqtSignal(int, str)
    progress_updated = pyqtSignal(int)
    vulnerability_found = pyqtSignal(str, str, str, str)
    log_event = pyqtSignal(str)
    stats_updated = pyqtSignal(str, int)
    site_structure_updated = pyqtSignal(list, list)

class ScanWorker:
    """
    –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –≤–æ—Ä–∫–µ—Ä –¥–ª—è —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –≤–µ–±-—Å–∞–π—Ç–æ–≤ –Ω–∞ —É—è–∑–≤–∏–º–æ—Å—Ç–∏.
    """

    def __init__(self, url: str, scan_types: List[str], user_id: int, username: Optional[str] = None,
                 max_depth: int = MAX_DEPTH, max_concurrent: int = 10, timeout: int = 10):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç ScanWorker —Å —É–∫–∞–∑–∞–Ω–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏.
        """
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å –≤—Ö–æ–¥–Ω—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
        if not is_safe_url(url):
            raise ValueError(f"–ù–µ–±–µ–∑–æ–ø–∞—Å–Ω—ã–π URL: {url}")
        
        if not validate_input_length(url, 1, 2048):
            raise ValueError(f"URL –∏–º–µ–µ—Ç –Ω–µ–¥–æ–ø—É—Å—Ç–∏–º—É—é –¥–ª–∏–Ω—É: {len(url)}")
        
        if not scan_types:
            raise ValueError("–¢–∏–ø—ã —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –Ω–µ–ø—É—Å—Ç—ã–º —Å–ø–∏—Å–∫–æ–º")
        
        if user_id <= 0:
            raise ValueError("ID –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–º —á–∏—Å–ª–æ–º")
        
        if username and not validate_input_length(username, 1, 50):
            raise ValueError("–ò–º—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –∏–º–µ–µ—Ç –Ω–µ–¥–æ–ø—É—Å—Ç–∏–º—É—é –¥–ª–∏–Ω—É")
        
        if max_depth < 1 or max_depth > 10:
            raise ValueError("–ì–ª—É–±–∏–Ω–∞ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ –æ—Ç 1 –¥–æ 10")
        
        if max_concurrent < 1 or max_concurrent > 20:
            raise ValueError("–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ –æ—Ç 1 –¥–æ 20")
        
        if timeout < 5 or timeout > 120:
            raise ValueError("–¢–∞–π–º–∞—É—Ç –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ –æ—Ç 5 –¥–æ 120 —Å–µ–∫—É–Ω–¥")
        
        # –†–µ–∂–∏–º—ã –∏ —Ñ–ª–∞–≥–∏ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è
        self._max_coverage_mode = False
        self._should_stop = False
        
        # –û—á–µ—Ä–µ–¥–∏ –∏ –º–Ω–æ–∂–µ—Å—Ç–≤–∞ –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ–º
        self.to_visit: asyncio.Queue[Tuple[str, int]] = asyncio.Queue()
        self.visited: Set[str] = set()
        self.in_progress: Set[str] = set()
        
        # –°—á–µ—Ç—á–∏–∫–∏ –∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        self.total_scanned_count = 0
        self.total_forms_count = 0
        self.total_vuln_count = 0
        self.scanned_forms_count = 0
        self.current_form_index = 0
        self.total_links_count = 0
        
        # –§–ª–∞–≥–∏ —Å–æ—Å—Ç–æ—è–Ω–∏—è —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è
        self.max_depth_reached = False
        self.scan_complete = False
        self.scan_started = False
        
        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è
        self.url = url
        self.scan_types = scan_types
        self.user_id = user_id
        self.username = username
        self.max_depth = max_depth
        self.max_concurrent = max_concurrent
        self.timeout = timeout
        self._is_paused = False
        
        # –û—Å–Ω–æ–≤–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        self.base_url = url
        self.current_url = ""
        
        # –ö—ç—à–∏ –∏ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞
        self.visited_urls: Set[str] = set()
        self.scanned_urls: Set[str] = set()
        self.unscanned_urls: Set[str] = set()
        self.all_scanned_urls: Set[str] = set()
        self.all_found_forms: List[Dict[str, Any]] = []
        self.scanned_form_hashes: Set[str] = set()
        self.html_cache: Dict[str, str] = {}
        self.dns_cache: Dict[str, str] = {}
        self.form_cache: Dict[str, Any] = {}
        self.url_cache: Set[str] = set()

        # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        self.vulnerabilities: Dict[str, List[Dict[str, Any]]] = {'sql': [], 'xss': [], 'csrf': []}
        self.scan_start_time = None
        self.scan_end_time = None
        self.current_depth = 0
        self.operation_count = 0
        self.memory_check_interval = 1000
        
        self.scan_completion_metrics: ScanCompletionMetrics = {
            'errors_encountered': 0,
            'urls_scanned': 0,
            'vulnerabilities_found': 0,
            'status': 'initialized'
        }

        # –°–∏–≥–Ω–∞–ª—ã –∏ —Å—Ç–∞—Ç—É—Å
        self.signals = ScanWorkerSignals()
        self._cancelled = False
        self.session = None
        self.start_time = 0

    @property
    def max_coverage_mode(self) -> bool:
        return self._max_coverage_mode

    @max_coverage_mode.setter
    def max_coverage_mode(self, value: bool) -> None:
        self._max_coverage_mode = value
        
    @property
    def should_stop(self) -> bool:
        return self._should_stop

    @should_stop.setter
    def should_stop(self, value: bool) -> None:
        self._should_stop = value

    async def _scan_sql_injection(self, url: str) -> None:
        """–°–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ SQL-–∏–Ω—ä–µ–∫—Ü–∏–∏."""
        try:
            await asyncio.sleep(0.1)  # –ò–º–∏—Ç–∞—Ü–∏—è —Ä–∞–±–æ—Ç—ã
            
            if any(keyword in url.lower() for keyword in ['login', 'search', 'id=', 'user=']):
                vulnerability = {
                    'type': 'sql',
                    'url': url,
                    'severity': 'high',
                    'description': '–í–æ–∑–º–æ–∂–Ω–∞—è SQL-–∏–Ω—ä–µ–∫—Ü–∏—è –≤ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞—Ö –∑–∞–ø—Ä–æ—Å–∞'
                }
                
                self.vulnerabilities['sql'].append(vulnerability)
                self.scan_completion_metrics['vulnerabilities_found'] += 1
                self.signals.vulnerability_found.emit(url, 'SQL Injection', vulnerability['description'], 'high')
                
        except Exception as e:
            logger.error(f"Error during SQL injection scan: {e}")
            self.scan_completion_metrics['errors_encountered'] += 1

    async def _scan_xss(self, url: str) -> None:
        """–°–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ XSS-—É—è–∑–≤–∏–º–æ—Å—Ç–∏."""
        try:
            await asyncio.sleep(0.1)  # –ò–º–∏—Ç–∞—Ü–∏—è —Ä–∞–±–æ—Ç—ã
            
            if any(keyword in url.lower() for keyword in ['comment', 'message', 'search', 'q=']):
                vulnerability = {
                    'type': 'xss',
                    'url': url,
                    'severity': 'medium',
                    'description': '–ü–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω–∞—è XSS-—É—è–∑–≤–∏–º–æ—Å—Ç—å –≤ —Ñ–æ—Ä–º–µ –∏–ª–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞—Ö'
                }
                
                self.vulnerabilities['xss'].append(vulnerability)
                self.scan_completion_metrics['vulnerabilities_found'] += 1
                self.signals.vulnerability_found.emit(url, 'XSS', vulnerability['description'], 'medium')
                
        except Exception as e:
            logger.error(f"Error during XSS scan: {e}")
            self.scan_completion_metrics['errors_encountered'] += 1

    async def _scan_csrf(self, url: str) -> None:
        """–°–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ CSRF-—É—è–∑–≤–∏–º–æ—Å—Ç–∏."""
        try:
            await asyncio.sleep(0.1)  # –ò–º–∏—Ç–∞—Ü–∏—è —Ä–∞–±–æ—Ç—ã
            
            if any(keyword in url.lower() for keyword in ['form', 'submit', 'transfer', 'delete']):
                vulnerability = {
                    'type': 'csrf',
                    'url': url,
                    'severity': 'medium',
                    'description': '–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç CSRF-—Ç–æ–∫–µ–Ω –≤ —Ñ–æ—Ä–º–µ'
                }
                
                self.vulnerabilities['csrf'].append(vulnerability)
                self.scan_completion_metrics['vulnerabilities_found'] += 1
                self.signals.vulnerability_found.emit(url, 'CSRF', vulnerability['description'], 'medium')
                
        except Exception as e:
            logger.error(f"Error during CSRF scan: {e}")
            self.scan_completion_metrics['errors_encountered'] += 1

    async def run_scan(self) -> Dict[str, Any]:
        """–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –∏ –≤–æ–∑–≤—Ä–∞—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤."""
        try:
            self.scan_start_time = datetime.now()
            logger.info(f"Starting scan of {self.base_url}")

            # –í—ã–ø–æ–ª–Ω—è–µ–º —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–∏–ø–∞ —É—è–∑–≤–∏–º–æ—Å—Ç–µ–π
            for scan_type in self.scan_types:
                if self.should_stop:
                    break
                    
                if scan_type == "sql":
                    await self._scan_sql_injection(self.base_url)
                elif scan_type == "xss":
                    await self._scan_xss(self.base_url)
                elif scan_type == "csrf":
                    await self._scan_csrf(self.base_url)

            self.scan_end_time = datetime.now()
            duration = (self.scan_end_time - self.scan_start_time).total_seconds()
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            results: Dict[str, Any] = {
                "url": self.base_url,
                "scan_types": self.scan_types,
                "start_time": self.scan_start_time.isoformat(),
                "end_time": self.scan_end_time.isoformat(),
                "duration": duration,
                "vulnerabilities": self.vulnerabilities
            }

            return results

        except Exception as e:
            logger.error(f"Error during scan: {e}")
            raise

    def _cleanup_caches(self):
        """–û—á–∏—Å—Ç–∫–∞ –∫—ç—à–µ–π –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –ø–∞–º—è—Ç—å—é."""
        self.html_cache.clear()
        self.dns_cache.clear()
        self.form_cache.clear()
        if hasattr(self, 'url_cache'):
            self.url_cache.clear()
        self.operation_count = 0
        logger.debug("Caches cleaned up")

    def update_stats(self):
        """–û–±–Ω–æ–≤–ª—è–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è."""
        try:
            # –ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—É—â–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è
            urls_found = len(self.visited_urls)
            urls_scanned = len(self.all_scanned_urls)
            forms_found = len(self.all_found_forms)
            forms_scanned = self.scanned_forms_count
            
            # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º —É—è–∑–≤–∏–º–æ—Å—Ç–∏
            total_vulns = (
                len(self.vulnerabilities.get('sql', [])) + 
                len(self.vulnerabilities.get('xss', [])) + 
                len(self.vulnerabilities.get('csrf', []))
            )
            
            self.signals.stats_updated.emit('urls_found', urls_found)
            self.signals.stats_updated.emit('urls_scanned', urls_scanned)
            self.signals.stats_updated.emit('forms_found', forms_found)
            self.signals.stats_updated.emit('forms_scanned', forms_scanned)
            self.signals.stats_updated.emit('vulnerabilities', total_vulns)
            
            # –û–±–Ω–æ–≤–ª—è–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å
            progress = self.calculate_progress()
            self.signals.progress_updated.emit(progress)
            
            # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º –æ—à–∏–±–∫–∏
            errors = self.scan_completion_metrics.get('errors_encountered', 0)
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–ª—è –æ—Ç–ø—Ä–∞–≤–∫–∏
            stats_data = {
                'urls_found': urls_found,
                'urls_scanned': urls_scanned,
                'forms_found': forms_found,
                'forms_scanned': forms_scanned,
                'vulnerabilities': total_vulns,
                'requests_sent': self.total_scanned_count,
                'errors': errors
            }
            
            # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∫–∞–∂–¥—ã–π —Å—á–µ—Ç—á–∏–∫
            for key, value in stats_data.items():
                try:
                    self.signals.stats_updated.emit(key, value)
                except Exception as signal_error:
                    logger.debug(f"Error emitting stat {key}: {signal_error}")

            # –í—Ä–µ–º—è —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è
            try:
                elapsed = 0
                current_time = time.time()
                
                if hasattr(self, 'start_time') and self.start_time:
                    elapsed = int(current_time - self.start_time)
                elif self.scan_start_time:
                    elapsed = int(current_time - self.scan_start_time.timestamp())
                
                # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –≤—Ä–µ–º—è
                hours = elapsed // 3600
                minutes = (elapsed % 3600) // 60
                seconds = elapsed % 60
                time_str = f"{hours:02d}:{minutes:02d}:{seconds:02d}"
                
                self.signals.stats_updated.emit('scan_time', time_str)
                
            except Exception as time_error:
                logger.debug(f"Error calculating scan time: {time_error}")
                self.signals.stats_updated.emit('scan_time', "00:00:00")
        
        except Exception as e:
            logger.error(f"Error in update_stats: {e}")

    def _manage_memory_usage(self):
        """–£–ø—Ä–∞–≤–ª—è–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –ø–∞–º—è—Ç–∏ —á–µ—Ä–µ–∑ –∫–æ–Ω—Ç—Ä–æ–ª—å –∫—ç—à–µ–π."""
        try:
            import psutil
            memory_percent = psutil.virtual_memory().percent
            
            if memory_percent > 80:
                # –û—á–∏—â–∞–µ–º –∫—ç—à–∏
                cache_dicts: List[Dict[str, Any]] = [self.html_cache, self.dns_cache, self.form_cache]
                for cache_dict in cache_dicts:
                    cache_dict.clear()
                
                cache_sets = [self.url_cache]
                for cache_set in cache_sets:
                    cache_set.clear()
                
                logger.warning(f"Memory usage {memory_percent}% > 80%. Cache sizes reduced and cleared.")
        except ImportError:
            pass  # psutil –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω
        except Exception as e:
            logger.debug(f"Error managing memory: {e}")

    def _check_memory_periodically(self):
        """–ü–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∏ –ø—Ä–æ–≤–µ—Ä—è–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏."""
        self.operation_count += 1
        if self.operation_count >= self.memory_check_interval:
            self._manage_memory_usage()
            self.operation_count = 0

    async def scan_url(self, url: str) -> Optional[str]:
        """–°–∫–∞–Ω–∏—Ä—É–µ—Ç —É–∫–∞–∑–∞–Ω–Ω—ã–π URL."""
        self._check_memory_periodically()

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫—ç—à–∞
        cache_key = f"scan_url{url}"
        cached_result = cache_manager.URL_PROCESSING_CACHE.get(cache_key)
        if cached_result is not None:
            logger.debug(f"Cache hit for {url}")
            return cached_result

        # –û–±—Ä–∞–±–æ—Ç–∫–∞ URL
        result = await self._process_url(url)

        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –∫—ç—à
        if result is not None:
            self.html_cache[url] = result

        self.update_stats()
        return result

    async def _process_url(self, url: str) -> Optional[str]:
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç URL –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç."""
        if self.should_stop or self._is_paused:
            return None
            
        try:
            timeout = aiohttp.ClientTimeout(total=self.timeout)
            async with aiohttp.ClientSession(timeout=timeout) as session:
                async with session.get(url) as response:
                    return await response.text()
        except Exception as e:
            logger.error(f"Error processing URL {url}: {e}")
            return None

    def stop(self):
        """–û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ."""
        self.should_stop = True
        logger.info(f"Stop signal sent for scan of {self.base_url}")

    def pause(self):
        """–ü—Ä–∏–æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ."""
        self._is_paused = True
        logger.info(f"Pause signal sent for scan of {self.base_url}")

    def resume(self):
        """–í–æ–∑–æ–±–Ω–æ–≤–ª—è–µ—Ç —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ."""
        self._is_paused = False
        logger.info(f"Resume signal sent for scan of {self.base_url}")

    def is_paused(self):
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –ª–∏ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ –ø–∞—É–∑–µ."""
        return self._is_paused

    def calculate_progress(self) -> int:
        """–í—ã—á–∏—Å–ª—è–µ—Ç –ø—Ä–æ–≥—Ä–µ—Å—Å —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è."""
        try:
            total = self.total_links_count
            processed = len(self.all_scanned_urls)
            return int((processed / total) * 100) if total > 0 else 0
        except Exception as e:
            logger.error(f"Error calculating progress: {e}")
            return 0

    def update_progress(self, current_url: str = "", current_depth: Optional[int] = None, queue_size: Optional[int] = None):
        """–û–±–Ω–æ–≤–ª—è–µ—Ç –ø—Ä–æ–≥—Ä–µ—Å—Å —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è."""
        try:
            if queue_size is None:
                queue_size = self.to_visit.qsize() if self.to_visit else 0
            
            progress = self.calculate_progress()
            
            # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —Å–∏–≥–Ω–∞–ª—ã –æ –ø—Ä–æ–≥—Ä–µ—Å—Å–µ
            self.signals.progress.emit(progress, current_url)
            self.signals.progress_updated.emit(progress)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –º–∞–∫—Å–∏–º–∞–ª—å–Ω—É—é –≥–ª—É–±–∏–Ω—É
            if current_depth is not None and current_depth >= self.max_depth:
                self.max_depth_reached = True
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø—Ä–æ–≥—Ä–µ—Å—Å–µ
            depth_info = f"{current_depth if current_depth is not None else self.current_depth}/{self.max_depth}"
            url_info = f"{len(self.all_scanned_urls)}/{self.total_links_count}"
            form_info = f"{self.scanned_forms_count}/{len(self.all_found_forms)}"
            
            progress_info = (
                f"Progress: {progress}% | "
                f"Depth: {depth_info} | "
                f"URL: {url_info} | "
                f"Forms: {form_info}"
            )
            
            if current_url:
                progress_info += f" | Processed URL: {current_url}"
            
            self.signals.log_event.emit(progress_info)
            self.update_stats()
            
        except Exception as e:
            logger.error(f"Error in update_progress: {e}")

    @staticmethod
    def get_form_hash(form_tag: Tag) -> str:
        """–°–æ–∑–¥–∞–µ—Ç —É–Ω–∏–∫–∞–ª—å–Ω—ã–π —Ö—ç—à –¥–ª—è —Ç–µ–≥–∞ —Ñ–æ—Ä–º—ã."""
        try:
            action = str(form_tag.get('action', '')).strip()
            method = str(form_tag.get('method', 'get')).lower().strip()

            inputs: List[str] = []
            for element in form_tag.find_all(['input', 'textarea', 'select', 'button']):
                inp_name = str(element.get('name', ''))
                inp_type = str(element.get('type', 'text'))
                if inp_name:
                    inputs.append(f"{element.name}-{inp_type}-{inp_name}")

            inputs.sort()
            form_representation = f"action:{action}|method:{method}|inputs:{','.join(inputs)}"
            return hashlib.sha256(form_representation.encode('utf-8')).hexdigest()

        except Exception as e:
            logger.error(f"Error creating form hash: {e}")
            return hashlib.sha256(str(time.time()).encode()).hexdigest()

    @staticmethod
    def is_same_domain(url: str, base_domain: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –ø—Ä–∏–Ω–∞–¥–ª–µ–∂–∏—Ç –ª–∏ URL –¥–∞–Ω–Ω–æ–º—É –¥–æ–º–µ–Ω—É."""
        try:
            if not url or not base_domain:
                return False
            
            parsed = urlparse(url)
            url_domain = parsed.netloc.lower().split(':')[0]
            base_domain = base_domain.lower().split(':')[0]
            
            return url_domain == base_domain or url_domain.endswith('.' + base_domain)
            
        except Exception as e:
            logger.error(f"Error checking domain {url} against {base_domain}: {e}")
            return False

    async def smart_request(self, session: aiohttp.ClientSession, method: str, url: str, 
                      retries: int = 2, **kwargs: Any) -> Optional[Tuple[aiohttp.ClientResponse, str]]:
        """–£–º–Ω—ã–π HTTP –∑–∞–ø—Ä–æ—Å —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫."""
        if self.should_stop or self._is_paused:
            return None

        if not session or not url:
            return None
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à
        cache_key = f"{method}:{url}:{hash(str(kwargs))}"
        cached_result = cache_manager.URL_PROCESSING_CACHE.get(cache_key)
        if cached_result is not None:
            return cached_result
        
        max_attempts = 3 if self.max_coverage_mode else retries
        
        for attempt in range(max_attempts):
            if self.should_stop:
                return None
                
            try:
                timeout = aiohttp.ClientTimeout(**HTTP_OPTIMIZATIONS['timeout'])
                headers = {**HTTP_OPTIMIZATIONS['headers'], **kwargs.get('headers', {})}
                
                async with session.request(method, url, timeout=timeout, headers=headers, **kwargs) as response:
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º Content-Type
                    content_type = response.headers.get('Content-Type', '').lower()
                    
                    if not any(t in content_type for t in ['html', 'text', 'json', 'xml', 'javascript']):
                        await response.read()
                        result = (response, "")
                    else:
                        try:
                            response_text = await response.text()
                        except UnicodeDecodeError:
                            response_text = await response.text(errors='replace')
                        
                        result = (response, response_text)
                    
                    cache_manager.URL_PROCESSING_CACHE.set(cache_key, result)
                    return result
                    
            except Exception as e:
                logger.warning(f"Request attempt {attempt + 1} failed for {url}: {e}")
                if attempt < max_attempts - 1:
                    await asyncio.sleep(1)
        
        self.unscanned_urls.add(url)
        return None

    async def crawl(self, session: aiohttp.ClientSession, semaphore: asyncio.Semaphore) -> None:
        """–ö—Ä–∞—É–ª–∏–Ω–≥ ‚Äî –æ–±—Ö–æ–¥ —Å–∞–π—Ç–∞, —Å–±–æ—Ä –≤—Å–µ—Ö —Å—Å—ã–ª–æ–∫ –∏ —Ñ–æ—Ä–º."""
        try:
            logger.info(f"Starting crawl for URL: {self.base_url}")
            self.signals.log_event.emit(f"üîç –ù–∞—á–∏–Ω–∞–µ–º –æ–±—Ö–æ–¥ —Å–∞–π—Ç–∞: {self.base_url}")

            # –û—á–∏—Å—Ç–∫–∞ –∫—ç—à–µ–π –∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –æ—á–µ—Ä–µ–¥–∏
            self.visited_urls.clear()
            self.scanned_urls.clear()
            self.all_scanned_urls.clear()
            self.all_found_forms.clear()
            self.scanned_form_hashes.clear()
            self.to_visit = asyncio.Queue()
            await self.to_visit.put((self.base_url, 0))
            self.total_links_count = 1

            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º results_by_type –ø–µ—Ä–µ–¥ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º
            results_by_type: Dict[str, List[Dict[str, Any]]] = {'sql': [], 'xss': [], 'csrf': []}

            # –ó–∞–ø—É—Å–∫ –æ–±—Ö–æ–¥–∞ —Å –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º–æ–º
            await self.crawl_and_scan_parallel(session, semaphore, self.base_url,
                                            results_by_type=results_by_type,
                                            visited_urls=self.visited_urls,
                                            scanned_urls=self.scanned_urls)
            logger.info(f"Crawling completed. Total URLs found: {len(self.visited_urls)}")
            self.signals.log_event.emit(f"‚úÖ –û–±—Ö–æ–¥ –∑–∞–≤–µ—Ä—à—ë–Ω. –ù–∞–π–¥–µ–Ω–æ URL: {len(self.visited_urls)}")

        except Exception as e:
            log_and_notify('error', f"Error in crawl: {e}")
            self.signals.log_event.emit(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ö–æ–¥–∞: {str(e)}")
            raise

    async def crawl_and_scan_parallel(self, session: aiohttp.ClientSession, semaphore: asyncio.Semaphore,
                                      start_url: str, results_by_type: Dict[str, List[Dict[str, Any]]], 
                                      visited_urls: Set[str], scanned_urls: Set[str]):
        """–ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–µ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ —Å –æ–±—Ö–æ–¥–æ–º —Å—Å—ã–ª–æ–∫."""
        try:
            logger.info(f"Starting crawl_and_scan_parallel for {start_url}")
            logger.info(f"Queue size at start: {self.to_visit.qsize() if self.to_visit else 0}")
            
            processed_count = 0
            stats_update_interval = 5  # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∫–∞–∂–¥—ã–µ 5 URL –¥–ª—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏

            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º URL –∏–∑ –æ—á–µ—Ä–µ–¥–∏
            logger.info(f"Starting to process URLs from queue. Queue size: {self.to_visit.qsize() if self.to_visit else 0}")
            while self.to_visit and not self.to_visit.empty() and not self.should_stop:
                try:
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–∞—É–∑—É –ø–µ—Ä–µ–¥ –æ–±—Ä–∞–±–æ—Ç–∫–æ–π URL
                    if self._is_paused:
                        await asyncio.sleep(0.1)
                        continue

                    url, current_depth = await self.to_visit.get()
                    processed_count += 1
                    logger.info(f"Processing URL {processed_count}: {url} at depth {current_depth}")

                    if self.should_stop:
                        logger.info("Received request to stop scanning. Finishing...")
                        break

                    if current_depth > self.max_depth:
                        logger.info(f"Reached maximum depth {self.max_depth} for {url} - SKIPPING")
                        continue

                    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º URL
                    await self._process_and_scan_url(session, semaphore, url, visited_urls, scanned_urls,
                                                   set(), results_by_type, self.to_visit, current_depth)

                    # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∏ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
                    if processed_count % stats_update_interval == 0:
                        self.update_stats()
                        # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —Å–∏–≥–Ω–∞–ª –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è —Å—Ç—Ä—É–∫—Ç—É—Ä—ã —Å–∞–π—Ç–∞
                        self.signals.site_structure_updated.emit(list(self.all_scanned_urls), self.all_found_forms)
                        # –í—ã–ø–æ–ª–Ω—è–µ–º —Å–±–æ—Ä–∫—É –º—É—Å–æ—Ä–∞ –¥–ª—è –æ—Å–≤–æ–±–æ–∂–¥–µ–Ω–∏—è –ø–∞–º—è—Ç–∏
                        gc.collect()

                except asyncio.CancelledError:
                    logger.info("Scanning task cancelled.")
                    break
                except Exception as e:
                    log_and_notify('error', f"Error in scanning task: {e}")

            logger.info(f"Main scanning loop completed. Processed {processed_count} URLs.")
            logger.info(f"Final queue size: {self.to_visit.qsize() if self.to_visit else 0}")
            logger.info(f"Max depth reached: {self.max_depth_reached}")
            
            # –§–∏–Ω–∞–ª—å–Ω–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
            self.update_stats()
            self.signals.site_structure_updated.emit(list(self.all_scanned_urls), self.all_found_forms)
            # –§–∏–Ω–∞–ª—å–Ω–∞—è —Å–±–æ—Ä–∫–∞ –º—É—Å–æ—Ä–∞
            gc.collect()

        except Exception as e:
            log_and_notify('error', f"Error in crawl_and_scan_parallel: {e}")

    async def _process_and_scan_url(self, session: aiohttp.ClientSession, semaphore: asyncio.Semaphore,
                              url: str, visited_urls: Set[str], scanned_urls: Set[str],
                              seen_urls: Set[str], results_by_type: Dict[str, List[Dict[str, Any]]],
                              to_visit: asyncio.Queue[Tuple[str, int]], current_depth: int) -> Tuple[Set[str], List[Tag]]:
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –∏ —Å–∫–∞–Ω–∏—Ä—É–µ—Ç –æ–¥–∏–Ω URL."""
        links: Set[str] = set()
        forms: List[Tag] = []

        if url in visited_urls or url in seen_urls:
            return set(), []

        if self._is_paused or self.should_stop:
            return set(), []

        # –î–æ–±–∞–≤–ª—è–µ–º URL —Ç–æ–ª—å–∫–æ –≤ seen_urls –Ω–∞ –Ω–∞—á–∞–ª—å–Ω–æ–º —ç—Ç–∞–ø–µ
        seen_urls.add(url)
        logger.info(f"Scanning URL: {url} at depth {current_depth}")

        try:
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å—Å—ã–ª–∫–∏ –∏ —Ñ–æ—Ä–º—ã —Å —Ç–µ–∫—É—â–µ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            links, forms = await self._extract_links_from_url(
                session, semaphore, url,
                urlparse(self.base_url).netloc,
                visited_urls,
                only_forms=False
            )
            
            # –î–æ–±–∞–≤–ª—è–µ–º URL –≤ visited_urls —Å—Ä–∞–∑—É –ø–æ—Å–ª–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è
            visited_urls.add(url)

            # –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—ã–µ —Ñ–æ—Ä–º—ã –≤ –æ–±—â–∏–π —Å–ø–∏—Å–æ–∫
            new_forms_count = 0
            for form in forms:
                form_hash = self.get_form_hash(form)
                if form_hash not in [f.get('hash') for f in self.all_found_forms]:
                    self.all_found_forms.append({
                        'form': form,
                        'url': url,
                        'hash': form_hash
                    })
                    new_forms_count += 1

            self.total_forms_count = len(self.all_found_forms)
            logger.info(f"Added {new_forms_count} new unique forms. Total forms: {self.total_forms_count}")

            # –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—ã–µ —Å—Å—ã–ª–∫–∏ –≤ –æ—á–µ—Ä–µ–¥—å
            logger.info(f"Found {len(links)} links on {url} at current depth {current_depth}")
            new_links_added = 0
            skipped_visited = 0
            skipped_file = 0

            for link in links:
                if link in visited_urls:
                    skipped_visited += 1
                    continue
                if link in seen_urls:
                    skipped_visited += 1
                    continue
                if is_file_url(link):
                    logger.info(f"SKIP_FILE: {link}")
                    skipped_file += 1
                    continue
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å URL –ø–µ—Ä–µ–¥ –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ–º –≤ –æ—á–µ—Ä–µ–¥—å
                if not is_safe_url(link):
                    logger.warning(f"SKIP_UNSAFE_URL: {link}")
                    continue
                new_depth = current_depth + 1
                await to_visit.put((link, new_depth))
                self.total_links_count += 1
                new_links_added += 1
                logger.info(f"ADD_LINK: {link} with depth {new_depth} (total_links_count={self.total_links_count})")
                # –ù–µ –¥–æ–±–∞–≤–ª—è–µ–º —Å—Å—ã–ª–∫—É –≤ visited_urls –∑–¥–µ—Å—å, —Ç–æ–ª—å–∫–æ –≤ seen_urls
                seen_urls.add(link)

            logger.info(f"Link processing summary: total={len(links)}, added={new_links_added}, skipped_visited={skipped_visited}, skipped_file={skipped_file}")
            logger.info(f"Added {new_links_added} new links to queue. Queue size after adding links: {to_visit.qsize() if to_visit else 0}")

            # –°–∫–∞–Ω–∏—Ä—É–µ–º —Ç–µ–∫—É—â–∏–π URL
            unique_forms = [f['form'] for f in self.all_found_forms if f.get('url') == url]
            logger.info(f"Found {len(unique_forms)} unique forms on {url}. Starting scan...")

            # –û–±–Ω–æ–≤–ª—è–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å –ø–æ—Å–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏ URL
            self.update_progress(
                url,
                current_depth,
                to_visit.qsize() if to_visit else 0
            )

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –¥–æ—Å—Ç–∏–≥–ª–∏ –ª–∏ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –≥–ª—É–±–∏–Ω—ã
            if current_depth >= self.max_depth:
                self.max_depth_reached = True
                logger.info(f"Maximum depth {self.max_depth} reached at URL: {url}")

            logger.info(f"About to scan_single_url for {url} at depth {current_depth}")
            await self.scan_single_url(
                session, semaphore, url,
                scanned_urls,
                results_by_type, to_visit,
                current_depth, unique_forms
            )

        except aiohttp.ClientError as e:
            log_and_notify('error', f"Client error accessing {url}: {e}")
            self.unscanned_urls.add(url)
        except asyncio.TimeoutError:
            logger.warning(f"Timeout accessing {url}")
            self.unscanned_urls.add(url)
        except (ValueError, TypeError, AttributeError) as e:
            log_and_notify('error', f"Data processing error for {url}: {e}")
            self.unscanned_urls.add(url)
        except Exception as e:
            log_and_notify('error', f"Unexpected error processing {url}: {e}")
            self.unscanned_urls.add(url)

        return links, forms

    async def _extract_links_from_url(self, session: aiohttp.ClientSession, semaphore: asyncio.Semaphore,
                                      url: str, base_domain: str, visited_urls: Optional[Set[str]] = None, 
                                      only_forms: bool = False) -> Tuple[Set[str], List[Tag]]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å—Å—ã–ª–∫–∏ –∏ —Ñ–æ—Ä–º—ã —Å —É–∫–∞–∑–∞–Ω–Ω–æ–≥–æ URL."""
        if visited_urls is None:
            visited_urls = set()
            
        found_links: Set[str] = set()
        found_forms: List[Tag] = []
        
        try:
            if self.should_stop or self._is_paused:
                return found_links, found_forms
            if not url:
                logger.warning("Attempted to extract links from empty URL")
                return found_links, found_forms

            async with semaphore:
                result = await self.smart_request(
                    session=session, 
                    method='GET', 
                    url=url, 
                    retries=2
                )

                if result is None:
                    logger.debug(f"No response received for URL: {url}")
                    return found_links, found_forms
                
                html_content = result[1]
                if not html_content:
                    logger.debug(f"Empty HTML content received for URL: {url}")
                    return found_links, found_forms
                
                try:
                    # –ò—Å–ø–æ–ª—å–∑—É–µ–º LRU-–∫—ç—à –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ HTML
                    soup = parse_html_cached(html_content)
                except Exception as parse_error:
                    log_and_notify('error', f"Failed to parse HTML from {url}: {parse_error}")
                    return found_links, found_forms

                # –ï—Å–ª–∏ only_forms=False, –∏—â–µ–º —Å—Å—ã–ª–∫–∏
                if not only_forms:
                    try:
                        for link in soup.find_all('a', href=True):
                            href = str(link.get('href', '')).strip()
                            if not href:
                                continue
                                
                            absolute_url = urljoin(url, href)
                            if self.is_same_domain(absolute_url, base_domain):
                                if absolute_url not in visited_urls:
                                    found_links.add(absolute_url)
                    except Exception as link_error:
                        log_and_notify('warning', f"Error processing links in {url}: {link_error}")

                # –ò—â–µ–º —Ñ–æ—Ä–º—ã
                try:
                    for form in soup.find_all('form'):
                        found_forms.append(form)
                except Exception as form_error:
                    log_and_notify('warning', f"Error extracting forms from {url}: {form_error}")

        except aiohttp.ClientError as client_error:
            log_and_notify('error', f"Network error while processing {url}: {client_error}")
        except Exception as e:
            log_and_notify('error', f"Unexpected error processing {url}: {e}")
            
        return found_links, found_forms

    async def scan_single_url(self, session: aiohttp.ClientSession, semaphore: asyncio.Semaphore,
                        url: str, scanned_urls: Set[str],
                        results_by_type: Dict[str, List[Dict[str, Any]]], 
                        to_visit: asyncio.Queue[Tuple[str, int]], current_depth: int, 
                        forms_to_scan: Optional[List[Tag]] = None):
        """–°–∫–∞–Ω–∏—Ä—É–µ—Ç –æ–¥–∏–Ω URL –Ω–∞ —É—è–∑–≤–∏–º–æ—Å—Ç–∏."""
        if forms_to_scan is None:
            forms_to_scan = []
            
        if url in scanned_urls:
            logger.info(f"URL {url} already in scanned_urls, skipping")
            return
        if self._is_paused:
            logger.info(f"Scan is paused, skipping URL {url}")
            return
            
        logger.info(f"Starting to scan URL: {url} at depth {current_depth}")
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º self.visited_urls –≤–º–µ—Å—Ç–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞
        if url in self.visited_urls:
            logger.info(f"URL {url} already in visited_urls, skipping")
            return


        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å–µ–º–∞—Ñ–æ—Ä –¥–ª—è –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º–∞
        async with semaphore:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ñ–ª–∞–≥–∏ —Å–Ω–æ–≤–∞ –ø–µ—Ä–µ–¥ –Ω–∞—á–∞–ª–æ–º –æ–±—Ä–∞–±–æ—Ç–∫–∏
            if self.should_stop or self._is_paused:
                return

            scanned_urls.add(url)
            self.visited_urls.add(url)
            self.all_scanned_urls.add(url)
            self.total_scanned_count += 1

            # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ—Å–ª–µ –¥–æ–±–∞–≤–ª–µ–Ω–∏—è URL –≤ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ
            self.update_stats()

            # –ï—Å–ª–∏ –Ω–µ –¥–æ—Å—Ç–∏–≥–ª–∏ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –≥–ª—É–±–∏–Ω—ã, –∏–∑–≤–ª–µ–∫–∞–µ–º —Å—Å—ã–ª–∫–∏ —Å —Ç–µ–∫—É—â–µ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            if current_depth < self.max_depth:
                logger.info(f"Extracting links from {url} at depth {current_depth} (max_depth: {self.max_depth})")
                try:
                    links, forms = await self._extract_links_from_url(
                        session, semaphore, url,
                        urlparse(self.base_url).netloc,
                        self.visited_urls,
                        only_forms=False
                    )

                    # –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—ã–µ —Ñ–æ—Ä–º—ã –≤ –æ–±—â–∏–π —Å–ø–∏—Å–æ–∫
                    new_forms_count = 0
                    for form in forms:
                        form_hash = self.get_form_hash(form)
                        if form_hash not in [f.get('hash') for f in self.all_found_forms]:
                            self.all_found_forms.append({
                                'form': form,
                                'url': url,
                                'hash': form_hash
                            })
                            new_forms_count += 1

                    self.total_forms_count = len(self.all_found_forms)
                    logger.info(f"Added {new_forms_count} new unique forms. Total forms: {self.total_forms_count}")

                    # –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—ã–µ —Å—Å—ã–ª–∫–∏ –≤ –æ—á–µ—Ä–µ–¥—å
                    logger.info(f"Found {len(links)} links on {url} at current depth {current_depth}")
                    new_links_added = 0
                    for link in links:
                        if link not in self.visited_urls:
                            if is_file_url(link):
                                logger.info(f"SKIP_FILE: {link}")
                                continue
                            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å URL –ø–µ—Ä–µ–¥ –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ–º –≤ –æ—á–µ—Ä–µ–¥—å
                            if not is_safe_url(link):
                                logger.warning(f"SKIP_UNSAFE_URL: {link}")
                                continue
                            new_depth = current_depth + 1
                            await to_visit.put((link, new_depth))
                            self.total_links_count += 1
                            new_links_added += 1
                            logger.info(f"ADD_LINK: {link} with depth {new_depth} (total_links_count={self.total_links_count})")
                    logger.info(f"Added {new_links_added} new links to queue. Queue size after adding links: {to_visit.qsize() if to_visit else 0}")
                    # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —Å–∏–≥–Ω–∞–ª –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è —Å—Ç—Ä—É–∫—Ç—É—Ä—ã —Å–∞–π—Ç–∞
                    self.signals.site_structure_updated.emit(list(self.all_scanned_urls), self.all_found_forms)

                    # –û–±–Ω–æ–≤–ª—è–µ–º —Ñ–æ—Ä–º—ã –¥–ª—è —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è —É—è–∑–≤–∏–º–æ—Å—Ç–µ–π
                    forms_to_scan = [f['form'] for f in self.all_found_forms if f['url'] == url]
                    logger.info(f"Found {len(forms_to_scan)} unique forms on {url}. Starting scan...")

                except Exception as e:
                    log_and_notify('error', f"Error extracting links from {url}: {e}")
            else:
                logger.info(f"Reached max depth {current_depth} for URL {url}, not extracting links")
                
        try:
            if forms_to_scan:
                new_forms_count = 0
                for form in forms_to_scan:
                    form_hash = self.get_form_hash(form)
                    if form_hash not in self.scanned_form_hashes:
                        self.scanned_form_hashes.add(form_hash)
                        new_forms_count += 1
                if new_forms_count > 0:
                    self.scanned_forms_count += new_forms_count
                    
                # --- –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∑–∞–¥–∞—á (batch gather) ---
                batch_size = min(3, self.max_concurrent)  # —É–º–µ–Ω—å—à–µ–Ω–æ –¥–æ 3 –¥–ª—è —Å–Ω–∏–∂–µ–Ω–∏—è –Ω–∞–≥—Ä—É–∑–∫–∏
                tasks: List[asyncio.Task[Any]] = []
                for scan_type in self.scan_types:
                    if self.should_stop:
                        return
                    if scan_type == 'sql':
                        tasks.append(asyncio.create_task(self.check_sql_injection(session, url, forms_to_scan)))
                    elif scan_type == 'xss':
                        tasks.append(asyncio.create_task(self.check_xss(session, url, forms_to_scan)))
                    elif scan_type == 'csrf':
                        tasks.append(asyncio.create_task(self.check_csrf(url, forms_to_scan)))
                # --- Batch gather ---
                for i in range(0, len(tasks), batch_size):
                    batch = tasks[i:i+batch_size]
                    results = await asyncio.gather(*batch, return_exceptions=True)
                    for j, result in enumerate(results):
                        if isinstance(result, Exception):
                            log_and_notify('error', f"Failed to scan URL {url}: {result}")
                        elif result:
                            scan_type = self.scan_types[i+j] if (i+j) < len(self.scan_types) else 'unknown'
                            if isinstance(result, dict):
                                self._process_scan_results(url, [result], [scan_type], results_by_type)
                                
            self.update_progress(url, current_depth, to_visit.qsize() if to_visit else 0)
            logger.info(f"Successfully scanned URL: {url} at depth {current_depth}")
            
        except Exception as e:
            log_and_notify('error', f"Failed to scan URL {url}: {e}")

    def _process_scan_results(self, url: str, results: List[Dict[str, Any]], 
                            scan_types_used: List[str], results_by_type: Dict[str, List[Dict[str, Any]]]):
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è."""
        try:
            for scan_type in scan_types_used:
                if scan_type in results_by_type:
                    results_by_type[scan_type].append({
                        'url': url,
                        'details': str(results),
                        'timestamp': datetime.now().isoformat()
                    })
                    
                    if scan_type not in self.vulnerabilities:
                        self.vulnerabilities[scan_type] = []
                    
                    self.vulnerabilities[scan_type].append({
                        'url': url,
                        'details': str(results),
                        'timestamp': datetime.now().isoformat()
                    })
                    
        except Exception as e:
            logger.error(f"Error processing scan results: {e}")

    async def check_sql_injection(self, session: aiohttp.ClientSession, url: str, 
                            forms: Optional[List[Tag]] = None) -> Optional[str]:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ SQL-–∏–Ω—ä–µ–∫—Ü–∏–∏."""
        if forms is None:
            forms = []
        try:
            # –¢–µ—Å—Ç–∏—Ä—É–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã URL
            if '?' in url:
                for payload in SAFE_SQL_PAYLOADS[:5]:  # –£–º–µ–Ω—å—à–µ–Ω–æ –¥–æ 5 –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
                    if self.should_stop:
                        return None
                        
                    test_url = self._inject_payload_into_url(url, payload)
                    result = await self.smart_request(session, 'GET', test_url)
                    
                    if result:
                        _, content = result
                        if any(pattern.search(content) for pattern in SQL_ERROR_PATTERNS):
                            return f"SQL injection vulnerability detected with payload: {payload}"
            
            # –¢–µ—Å—Ç–∏—Ä—É–µ–º —Ñ–æ—Ä–º—ã
            for form in forms:
                if self.should_stop:
                    return None
                    
                action = urljoin(url, str(form.get('action', '')))
                method = str(form.get('method', 'get')).upper()
                
                # –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Ñ–æ—Ä–º—ã
                form_data: Dict[str, str] = {}
                # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–ª–µ–π —Ñ–æ—Ä–º—ã –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
                input_elements = form.find_all('input')[:3]  # –ú–∞–∫—Å–∏–º—É–º 3 –ø–æ–ª—è
                for input_elem in input_elements:
                    input_name = str(input_elem.get('name', ''))
                    if input_name and input_elem.get('type') in ['text', 'password', 'email', 'search', 'url']:
                        form_data[input_name] = SAFE_SQL_PAYLOADS[0]  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–µ—Ä–≤—ã–π –ø—ç–π–ª–æ–∞–¥
                
                if form_data:
                    if method == 'POST':
                        result = await self.smart_request(session, 'POST', action, data=form_data)
                    else:
                        test_url = f"{action}?{urlencode(form_data)}"
                        result = await self.smart_request(session, 'GET', test_url)
                    
                    if result:
                        _, content = result
                        if any(pattern.search(content) for pattern in SQL_ERROR_PATTERNS):
                            return f"SQL injection vulnerability detected in form to {action}"
            
            return None
            
        except Exception as e:
            logger.error(f"Error in SQL injection check: {e}")
            return None


    async def check_xss(self, session: aiohttp.ClientSession, url: str, 
                       forms: List[Tag]) -> Optional[str]:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ XSS-—É—è–∑–≤–∏–º–æ—Å—Ç–∏."""
        try:
            # –¢–µ—Å—Ç–∏—Ä—É–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã URL
            if '?' in url:
                for payload in SAFE_XSS_PAYLOADS[:3]:  # –£–º–µ–Ω—å—à–µ–Ω–æ –¥–æ 3 –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
                    if self.should_stop:
                        return None
                        
                    test_url = self._inject_payload_into_url(url, payload)
                    result = await self.smart_request(session, 'GET', test_url)
                    
                    if result:
                        _, content = result
                        if payload in content:
                            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –ø—ç–π–ª–æ–∞–¥ –Ω–µ –±—ã–ª —ç–∫—Ä–∞–Ω–∏—Ä–æ–≤–∞–Ω
                            from bs4 import BeautifulSoup as BS
                            soup = BS(content, 'html.parser')
                            scripts = soup.find_all('script')
                            for script in scripts:
                                if script.string and payload in script.string:
                                    return f"XSS vulnerability detected with payload: {payload}"
            
            # –¢–µ—Å—Ç–∏—Ä—É–µ–º —Ñ–æ—Ä–º—ã
            for form in forms:
                if self.should_stop:
                    return None
                    
                action = urljoin(url, str(form.get('action', '')))
                method = str(form.get('method', 'get')).upper()
                
                # –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Ñ–æ—Ä–º—ã
                form_data: Dict[str, str] = {}
                # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–ª–µ–π —Ñ–æ—Ä–º—ã –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
                input_elements = form.find_all('input')[:3]  # –ú–∞–∫—Å–∏–º—É–º 3 –ø–æ–ª—è
                for input_elem in input_elements:
                    input_name = str(input_elem.get('name', ''))
                    if input_name and input_elem.get('type') in ['text', 'password', 'email', 'search', 'url']:
                        form_data[input_name] = SAFE_XSS_PAYLOADS[0]  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–µ—Ä–≤—ã–π –ø—ç–π–ª–æ–∞–¥
                
                if form_data:
                    if method == 'POST':
                        result = await self.smart_request(session, 'POST', action, data=form_data)
                    else:
                        test_url = f"{action}?{urlencode(form_data)}"
                        result = await self.smart_request(session, 'GET', test_url)
                    
                    if result:
                        _, content = result
                        if SAFE_XSS_PAYLOADS[0] in content:
                            return f"XSS vulnerability detected in form to {action}"
            
            return None
            
        except Exception as e:
            logger.error(f"Error in XSS check: {e}")
            return None

    async def check_csrf(self, url: str, forms: List[Tag]) -> Optional[str]:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ CSRF-—É—è–∑–≤–∏–º–æ—Å—Ç–∏."""
        try:
            known_csrf_token_names = {
                'csrf_token', 'csrfmiddlewaretoken', 'authenticity_token',
                '_csrf', '_token', '__requestverificationtoken', 'xsrf_token'
            }

            vulnerable_form_actions: List[str] = []
            
            for form in forms:
                try:
                    action = urljoin(url, str(form.get('action', '')))
                    form_method = str(form.get('method', 'get')).upper()

                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–æ–ª—å–∫–æ POST —Ñ–æ—Ä–º—ã
                    if form_method == 'POST':
                        # –ò—â–µ–º —Å–∫—Ä—ã—Ç—ã–µ –ø–æ–ª—è –≤ —Ñ–æ—Ä–º–µ
                        hidden_fields = form.find_all('input', type='hidden')
                        form_has_csrf_token = False
                        
                        for field in hidden_fields:
                            field_name = str(field.get('name', '')).lower()
                            if field_name in known_csrf_token_names:
                                form_has_csrf_token = True
                                break

                        # –ï—Å–ª–∏ —Ñ–æ—Ä–º–∞ –Ω–µ –∏–º–µ–µ—Ç CSRF —Ç–æ–∫–µ–Ω–∞, —Å—á–∏—Ç–∞–µ–º –µ—ë —É—è–∑–≤–∏–º–æ–π
                        if not form_has_csrf_token:
                            vulnerable_form_actions.append(action)
                            
                except Exception as e:
                    logger.warning(f"Error processing form in CSRF check: {e}")
                    continue

            if vulnerable_form_actions:
                unique_actions = sorted(list(set(vulnerable_form_actions)))
                result = f"Potential CSRF in POST forms to: {', '.join(unique_actions[:3])}"  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –≤—ã–≤–æ–¥
                return result
            
            return None
            
        except Exception as e:
            log_and_notify('error', f"Error in check_csrf: {e}")
            return None

    def _inject_payload_into_url(self, url: str, payload: str) -> str:
        """–í–Ω–µ–¥—Ä—è–µ—Ç –ø—ç–π–ª–æ–∞–¥ –≤ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã URL."""
        parsed = urlparse(url)
        query_params = parse_qs(parsed.query)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –ø—ç–π–ª–æ–∞–¥ –∫ –∫–∞–∂–¥–æ–º—É –ø–∞—Ä–∞–º–µ—Ç—Ä—É
        injected_params: Dict[str, List[str]] = {}
        for key, values in query_params.items():
            injected_params[key] = [f"{value}{payload}" for value in values]
        
        new_query = urlencode(injected_params, doseq=True)
        return parsed._replace(query=new_query).geturl()

    async def scan(self) -> Dict[str, Any]:
        """–û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –¥–ª—è –∑–∞–ø—É—Å–∫–∞ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è."""
        try:
            logger.info(f"Starting scan for URL: {self.base_url}")
            self.scan_start_time = datetime.now()
            self.start_time = time.time()
            
            self.signals.log_event.emit(f"üöÄ –ù–∞—á–∏–Ω–∞–µ–º —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ: {self.base_url} (–≥–ª—É–±–∏–Ω–∞: {self.max_depth})")
            
            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è
            self.visited_urls.clear()
            self.scanned_urls.clear()
            self.all_scanned_urls.clear()
            self.all_found_forms.clear()
            self.scanned_form_hashes.clear()
            
            self.to_visit = asyncio.Queue()
            
            await self.to_visit.put((self.base_url, 0))
            self.total_links_count = 1
            
            # –û—Å–Ω–æ–≤–Ω–æ–µ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ
            timeout = aiohttp.ClientTimeout(**HTTP_OPTIMIZATIONS['timeout'])
            
            async with aiohttp.ClientSession(timeout=timeout) as session:
                semaphore = asyncio.Semaphore(self.max_concurrent)
                self.session = session
                
                # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º scan_types
                scan_types_lower: List[str] = []
                for scan_type in self.scan_types:
                    if 'sql' in scan_type.lower():
                        scan_types_lower.append('sql')
                    elif 'xss' in scan_type.lower():
                        scan_types_lower.append('xss')
                    elif 'csrf' in scan_type.lower():
                        scan_types_lower.append('csrf')
                
                if not scan_types_lower:
                    scan_types_lower = ['sql', 'xss', 'csrf']
                
                self.scan_types = scan_types_lower
                
                # –í—ã–ø–æ–ª–Ω—è–µ–º —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ
                results_by_type: Dict[str, List[Dict[str, Any]]] = {'sql': [], 'xss': [], 'csrf': []}
                visited_urls: Set[str] = set()
                scanned_urls: Set[str] = set()
                
                await self.crawl_and_scan_parallel(session, semaphore, self.base_url, 
                                                 results_by_type, visited_urls, scanned_urls)
            
            # –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ
            self.scan_end_time = datetime.now()
            scan_duration = (self.scan_end_time - self.scan_start_time).total_seconds()
            
            if self.should_stop:
                status = 'stopped_by_user'
                self.signals.log_event.emit(f"‚èπÔ∏è –°–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º. –ü—Ä–æ—Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–æ URL: {self.total_scanned_count}")
            else:
                status = 'completed'
                self.signals.log_event.emit(f"‚úÖ –°–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ –∑–∞ {scan_duration:.2f}—Å")
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            result: Dict[str, Any] = {
                'url': self.base_url,
                'scan_types': self.scan_types,
                'duration': scan_duration,
                'total_urls_scanned': len(self.all_scanned_urls),
                'total_forms_scanned': self.scanned_forms_count,
                'vulnerabilities': self.vulnerabilities,
                'timestamp': datetime.now().isoformat(),
                'total_urls_discovered': self.total_links_count,
                'unscanned_urls': list(self.unscanned_urls),
                'status': status
            }
            
            total_vulnerabilities = sum(len(vulns) for vulns in self.vulnerabilities.values())
            self.signals.log_event.emit(f"üìä –ü—Ä–æ—Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–æ URL: {len(self.all_scanned_urls)}, —Ñ–æ—Ä–º: {self.scanned_forms_count}, —É—è–∑–≤–∏–º–æ—Å—Ç–µ–π: {total_vulnerabilities}")
            
            self.update_stats()
            return result
            
        except Exception as e:
            logger.error(f"Error in scan method: {e}")
            self.signals.log_event.emit(f"‚ùå –û—à–∏–±–∫–∞ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è: {str(e)}")
            
            return {
                'url': self.base_url,
                'scan_types': self.scan_types,
                'duration': 0,
                'total_urls_scanned': 0,
                'total_forms_scanned': 0,
                'vulnerabilities': {'sql': [], 'xss': [], 'csrf': []},
                'timestamp': get_local_timestamp(),
                'error': str(e),
                'total_urls_discovered': 0,
                'unscanned_urls': list(self.unscanned_urls),
                'status': 'failed'
            }

    async def save_results(self):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö."""
        try:
            scan_duration = time.time() - self.start_time if hasattr(self, 'start_time') and self.start_time > 0 else 0
            results: List[Dict[str, Any]] = []
            
            for vuln_type, vulns in self.vulnerabilities.items():
                for vuln in vulns:
                    results.append({
                        'type': vuln_type,
                        'url': vuln.get('url', self.base_url),
                        'details': vuln.get('details', ''),
                        'severity': vuln.get('severity', 'medium')
                    })
            
            scan_type = "comprehensive" if len(self.scan_types) > 1 else self.scan_types[0] if self.scan_types else "general"
            completion_status = getattr(self, 'scan_completion_metrics', {}).get('completion_status', 'unknown')
            if completion_status == 'stopped_by_user':
                scan_type += "_partial"
                
            success = db.save_scan_async(
                user_id=self.user_id,
                url=self.base_url,
                results=results,
                scan_type=scan_type,
                scan_duration=scan_duration
            )
            
            if success:
                logger.info(f"Scan results saved successfully for user {self.user_id}")
            else:
                logger.error("Failed to save scan results")
                
            # –û—á–∏—Å—Ç–∫–∞ –∫—ç—à–µ–π
            if hasattr(parse_html_cached, 'cache_info') and hasattr(parse_html_cached, 'cache_clear'):
                parse_html_cached.cache_clear()
            cache_manager.cleanup_all()
            gc.collect()
            
        except Exception as e:
            logger.error(f"Error saving scan results: {e}")
            self.signals.log_event.emit(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {str(e)}")